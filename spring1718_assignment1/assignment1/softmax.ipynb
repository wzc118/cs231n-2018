{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n",
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.369612\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -2.381104 analytic: -2.381104, relative error: 4.753724e-09\n",
      "numerical: -3.929688 analytic: -3.929688, relative error: 1.463066e-08\n",
      "numerical: -0.986944 analytic: -0.986944, relative error: 8.582509e-09\n",
      "numerical: 3.012227 analytic: 3.012227, relative error: 2.724001e-08\n",
      "numerical: 0.403435 analytic: 0.403435, relative error: 6.633683e-09\n",
      "numerical: 2.033160 analytic: 2.033160, relative error: 1.305114e-08\n",
      "numerical: 2.117342 analytic: 2.117342, relative error: 7.437090e-09\n",
      "numerical: 0.384063 analytic: 0.384063, relative error: 1.666172e-08\n",
      "numerical: -0.698313 analytic: -0.698313, relative error: 6.355525e-08\n",
      "numerical: 0.356983 analytic: 0.356983, relative error: 1.291578e-08\n",
      "numerical: 1.331489 analytic: 1.331489, relative error: 6.511806e-09\n",
      "numerical: 2.393722 analytic: 2.393722, relative error: 6.407577e-09\n",
      "numerical: -0.952240 analytic: -0.952240, relative error: 3.833282e-08\n",
      "numerical: -0.923879 analytic: -0.923879, relative error: 5.712278e-08\n",
      "numerical: 2.028698 analytic: 2.028698, relative error: 1.057789e-08\n",
      "numerical: -9.298524 analytic: -9.298524, relative error: 1.322754e-08\n",
      "numerical: 2.030117 analytic: 2.030117, relative error: 2.870972e-08\n",
      "numerical: 4.490966 analytic: 4.490966, relative error: 2.496740e-09\n",
      "numerical: -0.868690 analytic: -0.868690, relative error: 2.707349e-08\n",
      "numerical: 1.785737 analytic: 1.785737, relative error: 1.437330e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.369612e+00 computed in 0.013363s\n",
      "vectorized loss: 2.369612e+00 computed in 0.006477s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 769.426641\n",
      "iteration 100 / 1500: loss 282.635036\n",
      "iteration 200 / 1500: loss 104.597452\n",
      "iteration 300 / 1500: loss 39.667411\n",
      "iteration 400 / 1500: loss 15.836955\n",
      "iteration 500 / 1500: loss 7.140228\n",
      "iteration 600 / 1500: loss 3.875939\n",
      "iteration 700 / 1500: loss 2.811674\n",
      "iteration 800 / 1500: loss 2.301405\n",
      "iteration 900 / 1500: loss 2.142887\n",
      "iteration 1000 / 1500: loss 2.111433\n",
      "iteration 1100 / 1500: loss 2.094760\n",
      "iteration 1200 / 1500: loss 2.103948\n",
      "iteration 1300 / 1500: loss 2.119964\n",
      "iteration 1400 / 1500: loss 2.109470\n",
      "iteration 0 / 1500: loss 844.192364\n",
      "iteration 100 / 1500: loss 280.562574\n",
      "iteration 200 / 1500: loss 94.249319\n",
      "iteration 300 / 1500: loss 32.561380\n",
      "iteration 400 / 1500: loss 12.192581\n",
      "iteration 500 / 1500: loss 5.420638\n",
      "iteration 600 / 1500: loss 3.182032\n",
      "iteration 700 / 1500: loss 2.506315\n",
      "iteration 800 / 1500: loss 2.227395\n",
      "iteration 900 / 1500: loss 2.083812\n",
      "iteration 1000 / 1500: loss 2.102457\n",
      "iteration 1100 / 1500: loss 2.104614\n",
      "iteration 1200 / 1500: loss 2.081789\n",
      "iteration 1300 / 1500: loss 2.056186\n",
      "iteration 1400 / 1500: loss 2.164610\n",
      "iteration 0 / 1500: loss 919.384445\n",
      "iteration 100 / 1500: loss 276.022224\n",
      "iteration 200 / 1500: loss 84.132199\n",
      "iteration 300 / 1500: loss 26.636620\n",
      "iteration 400 / 1500: loss 9.441895\n",
      "iteration 500 / 1500: loss 4.293388\n",
      "iteration 600 / 1500: loss 2.826894\n",
      "iteration 700 / 1500: loss 2.330211\n",
      "iteration 800 / 1500: loss 2.156868\n",
      "iteration 900 / 1500: loss 2.159982\n",
      "iteration 1000 / 1500: loss 2.030465\n",
      "iteration 1100 / 1500: loss 2.136611\n",
      "iteration 1200 / 1500: loss 2.100338\n",
      "iteration 1300 / 1500: loss 2.099580\n",
      "iteration 1400 / 1500: loss 2.183865\n",
      "iteration 0 / 1500: loss 1010.010852\n",
      "iteration 100 / 1500: loss 274.551105\n",
      "iteration 200 / 1500: loss 75.809509\n",
      "iteration 300 / 1500: loss 22.034796\n",
      "iteration 400 / 1500: loss 7.537620\n",
      "iteration 500 / 1500: loss 3.600370\n",
      "iteration 600 / 1500: loss 2.460824\n",
      "iteration 700 / 1500: loss 2.228813\n",
      "iteration 800 / 1500: loss 2.133047\n",
      "iteration 900 / 1500: loss 2.127220\n",
      "iteration 1000 / 1500: loss 2.089047\n",
      "iteration 1100 / 1500: loss 2.126700\n",
      "iteration 1200 / 1500: loss 2.077451\n",
      "iteration 1300 / 1500: loss 2.091061\n",
      "iteration 1400 / 1500: loss 2.091736\n",
      "iteration 0 / 1500: loss 1085.939369\n",
      "iteration 100 / 1500: loss 266.998638\n",
      "iteration 200 / 1500: loss 66.871646\n",
      "iteration 300 / 1500: loss 17.931365\n",
      "iteration 400 / 1500: loss 5.980609\n",
      "iteration 500 / 1500: loss 3.054239\n",
      "iteration 600 / 1500: loss 2.356726\n",
      "iteration 700 / 1500: loss 2.167185\n",
      "iteration 800 / 1500: loss 2.118551\n",
      "iteration 900 / 1500: loss 2.174921\n",
      "iteration 1000 / 1500: loss 2.136620\n",
      "iteration 1100 / 1500: loss 2.108650\n",
      "iteration 1200 / 1500: loss 2.125512\n",
      "iteration 1300 / 1500: loss 2.105357\n",
      "iteration 1400 / 1500: loss 2.086223\n",
      "iteration 0 / 1500: loss 1156.158972\n",
      "iteration 100 / 1500: loss 256.898876\n",
      "iteration 200 / 1500: loss 58.568266\n",
      "iteration 300 / 1500: loss 14.582466\n",
      "iteration 400 / 1500: loss 4.923482\n",
      "iteration 500 / 1500: loss 2.711774\n",
      "iteration 600 / 1500: loss 2.225456\n",
      "iteration 700 / 1500: loss 2.197612\n",
      "iteration 800 / 1500: loss 2.122232\n",
      "iteration 900 / 1500: loss 2.138019\n",
      "iteration 1000 / 1500: loss 2.101853\n",
      "iteration 1100 / 1500: loss 2.180584\n",
      "iteration 1200 / 1500: loss 2.142269\n",
      "iteration 1300 / 1500: loss 2.085450\n",
      "iteration 1400 / 1500: loss 2.073046\n",
      "iteration 0 / 1500: loss 1228.816150\n",
      "iteration 100 / 1500: loss 247.245598\n",
      "iteration 200 / 1500: loss 51.057681\n",
      "iteration 300 / 1500: loss 11.934311\n",
      "iteration 400 / 1500: loss 4.124920\n",
      "iteration 500 / 1500: loss 2.549129\n",
      "iteration 600 / 1500: loss 2.211385\n",
      "iteration 700 / 1500: loss 2.105764\n",
      "iteration 800 / 1500: loss 2.119485\n",
      "iteration 900 / 1500: loss 2.111736\n",
      "iteration 1000 / 1500: loss 2.103374\n",
      "iteration 1100 / 1500: loss 2.142231\n",
      "iteration 1200 / 1500: loss 2.139246\n",
      "iteration 1300 / 1500: loss 2.141355\n",
      "iteration 1400 / 1500: loss 2.116941\n",
      "iteration 0 / 1500: loss 1319.149539\n",
      "iteration 100 / 1500: loss 240.077790\n",
      "iteration 200 / 1500: loss 45.138151\n",
      "iteration 300 / 1500: loss 9.913797\n",
      "iteration 400 / 1500: loss 3.527014\n",
      "iteration 500 / 1500: loss 2.392865\n",
      "iteration 600 / 1500: loss 2.149201\n",
      "iteration 700 / 1500: loss 2.127475\n",
      "iteration 800 / 1500: loss 2.131411\n",
      "iteration 900 / 1500: loss 2.135429\n",
      "iteration 1000 / 1500: loss 2.130111\n",
      "iteration 1100 / 1500: loss 2.116559\n",
      "iteration 1200 / 1500: loss 2.141644\n",
      "iteration 1300 / 1500: loss 2.156932\n",
      "iteration 1400 / 1500: loss 2.113369\n",
      "iteration 0 / 1500: loss 1390.066196\n",
      "iteration 100 / 1500: loss 229.073246\n",
      "iteration 200 / 1500: loss 39.157179\n",
      "iteration 300 / 1500: loss 8.173641\n",
      "iteration 400 / 1500: loss 3.147681\n",
      "iteration 500 / 1500: loss 2.313538\n",
      "iteration 600 / 1500: loss 2.186063\n",
      "iteration 700 / 1500: loss 2.102640\n",
      "iteration 800 / 1500: loss 2.110019\n",
      "iteration 900 / 1500: loss 2.155087\n",
      "iteration 1000 / 1500: loss 2.081040\n",
      "iteration 1100 / 1500: loss 2.176453\n",
      "iteration 1200 / 1500: loss 2.150605\n",
      "iteration 1300 / 1500: loss 2.126688\n",
      "iteration 1400 / 1500: loss 2.067114\n",
      "iteration 0 / 1500: loss 1466.003227\n",
      "iteration 100 / 1500: loss 218.303120\n",
      "iteration 200 / 1500: loss 34.048670\n",
      "iteration 300 / 1500: loss 6.911195\n",
      "iteration 400 / 1500: loss 2.854467\n",
      "iteration 500 / 1500: loss 2.248446\n",
      "iteration 600 / 1500: loss 2.176610\n",
      "iteration 700 / 1500: loss 2.104683\n",
      "iteration 800 / 1500: loss 2.123741\n",
      "iteration 900 / 1500: loss 2.151505\n",
      "iteration 1000 / 1500: loss 2.084375\n",
      "iteration 1100 / 1500: loss 2.135468\n",
      "iteration 1200 / 1500: loss 2.141644\n",
      "iteration 1300 / 1500: loss 2.150091\n",
      "iteration 1400 / 1500: loss 2.150846\n",
      "iteration 0 / 1500: loss 768.183800\n",
      "iteration 100 / 1500: loss 188.537105\n",
      "iteration 200 / 1500: loss 47.675032\n",
      "iteration 300 / 1500: loss 13.198198\n",
      "iteration 400 / 1500: loss 4.862367\n",
      "iteration 500 / 1500: loss 2.728031\n",
      "iteration 600 / 1500: loss 2.254391\n",
      "iteration 700 / 1500: loss 2.126426\n",
      "iteration 800 / 1500: loss 2.095780\n",
      "iteration 900 / 1500: loss 2.139459\n",
      "iteration 1000 / 1500: loss 2.122955\n",
      "iteration 1100 / 1500: loss 2.103172\n",
      "iteration 1200 / 1500: loss 2.042240\n",
      "iteration 1300 / 1500: loss 2.129713\n",
      "iteration 1400 / 1500: loss 2.070827\n",
      "iteration 0 / 1500: loss 850.673272\n",
      "iteration 100 / 1500: loss 181.788040\n",
      "iteration 200 / 1500: loss 40.262972\n",
      "iteration 300 / 1500: loss 10.210321\n",
      "iteration 400 / 1500: loss 3.779491\n",
      "iteration 500 / 1500: loss 2.503457\n",
      "iteration 600 / 1500: loss 2.166399\n",
      "iteration 700 / 1500: loss 2.171315\n",
      "iteration 800 / 1500: loss 2.171796\n",
      "iteration 900 / 1500: loss 2.138407\n",
      "iteration 1000 / 1500: loss 2.134327\n",
      "iteration 1100 / 1500: loss 2.066014\n",
      "iteration 1200 / 1500: loss 2.089686\n",
      "iteration 1300 / 1500: loss 2.129311\n",
      "iteration 1400 / 1500: loss 2.102314\n",
      "iteration 0 / 1500: loss 934.831120\n",
      "iteration 100 / 1500: loss 173.836880\n",
      "iteration 200 / 1500: loss 33.737990\n",
      "iteration 300 / 1500: loss 7.931346\n",
      "iteration 400 / 1500: loss 3.232270\n",
      "iteration 500 / 1500: loss 2.275010\n",
      "iteration 600 / 1500: loss 2.122832\n",
      "iteration 700 / 1500: loss 2.054105\n",
      "iteration 800 / 1500: loss 2.132428\n",
      "iteration 900 / 1500: loss 2.126265\n",
      "iteration 1000 / 1500: loss 2.106259\n",
      "iteration 1100 / 1500: loss 2.136070\n",
      "iteration 1200 / 1500: loss 2.028678\n",
      "iteration 1300 / 1500: loss 2.119805\n",
      "iteration 1400 / 1500: loss 2.101828\n",
      "iteration 0 / 1500: loss 1019.595804\n",
      "iteration 100 / 1500: loss 164.666168\n",
      "iteration 200 / 1500: loss 28.066120\n",
      "iteration 300 / 1500: loss 6.280848\n",
      "iteration 400 / 1500: loss 2.749952\n",
      "iteration 500 / 1500: loss 2.226866\n",
      "iteration 600 / 1500: loss 2.121654\n",
      "iteration 700 / 1500: loss 2.109687\n",
      "iteration 800 / 1500: loss 2.142285\n",
      "iteration 900 / 1500: loss 2.131519\n",
      "iteration 1000 / 1500: loss 2.095536\n",
      "iteration 1100 / 1500: loss 2.093215\n",
      "iteration 1200 / 1500: loss 2.109305\n",
      "iteration 1300 / 1500: loss 2.172845\n",
      "iteration 1400 / 1500: loss 2.139724\n",
      "iteration 0 / 1500: loss 1075.794670\n",
      "iteration 100 / 1500: loss 151.143732\n",
      "iteration 200 / 1500: loss 22.816158\n",
      "iteration 300 / 1500: loss 4.959158\n",
      "iteration 400 / 1500: loss 2.545501\n",
      "iteration 500 / 1500: loss 2.192137\n",
      "iteration 600 / 1500: loss 2.131383\n",
      "iteration 700 / 1500: loss 2.091275\n",
      "iteration 800 / 1500: loss 2.130955\n",
      "iteration 900 / 1500: loss 2.122704\n",
      "iteration 1000 / 1500: loss 2.117844\n",
      "iteration 1100 / 1500: loss 2.098520\n",
      "iteration 1200 / 1500: loss 2.079466\n",
      "iteration 1300 / 1500: loss 2.079041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.119387\n",
      "iteration 0 / 1500: loss 1161.136382\n",
      "iteration 100 / 1500: loss 141.552742\n",
      "iteration 200 / 1500: loss 18.980463\n",
      "iteration 300 / 1500: loss 4.215259\n",
      "iteration 400 / 1500: loss 2.375620\n",
      "iteration 500 / 1500: loss 2.111437\n",
      "iteration 600 / 1500: loss 2.111273\n",
      "iteration 700 / 1500: loss 2.053870\n",
      "iteration 800 / 1500: loss 2.128160\n",
      "iteration 900 / 1500: loss 2.132657\n",
      "iteration 1000 / 1500: loss 2.102310\n",
      "iteration 1100 / 1500: loss 2.119709\n",
      "iteration 1200 / 1500: loss 2.092619\n",
      "iteration 1300 / 1500: loss 2.043748\n",
      "iteration 1400 / 1500: loss 2.091921\n",
      "iteration 0 / 1500: loss 1242.325521\n",
      "iteration 100 / 1500: loss 131.872866\n",
      "iteration 200 / 1500: loss 15.711655\n",
      "iteration 300 / 1500: loss 3.559138\n",
      "iteration 400 / 1500: loss 2.309200\n",
      "iteration 500 / 1500: loss 2.120278\n",
      "iteration 600 / 1500: loss 2.097180\n",
      "iteration 700 / 1500: loss 2.144783\n",
      "iteration 800 / 1500: loss 2.151560\n",
      "iteration 900 / 1500: loss 2.074874\n",
      "iteration 1000 / 1500: loss 2.083273\n",
      "iteration 1100 / 1500: loss 2.085516\n",
      "iteration 1200 / 1500: loss 2.129348\n",
      "iteration 1300 / 1500: loss 2.123763\n",
      "iteration 1400 / 1500: loss 2.120070\n",
      "iteration 0 / 1500: loss 1330.543391\n",
      "iteration 100 / 1500: loss 122.680632\n",
      "iteration 200 / 1500: loss 13.075828\n",
      "iteration 300 / 1500: loss 3.100028\n",
      "iteration 400 / 1500: loss 2.253218\n",
      "iteration 500 / 1500: loss 2.128791\n",
      "iteration 600 / 1500: loss 2.156335\n",
      "iteration 700 / 1500: loss 2.116801\n",
      "iteration 800 / 1500: loss 2.128820\n",
      "iteration 900 / 1500: loss 2.162822\n",
      "iteration 1000 / 1500: loss 2.123724\n",
      "iteration 1100 / 1500: loss 2.122526\n",
      "iteration 1200 / 1500: loss 2.127410\n",
      "iteration 1300 / 1500: loss 2.104244\n",
      "iteration 1400 / 1500: loss 2.179026\n",
      "iteration 0 / 1500: loss 1408.472237\n",
      "iteration 100 / 1500: loss 112.927007\n",
      "iteration 200 / 1500: loss 10.880800\n",
      "iteration 300 / 1500: loss 2.793949\n",
      "iteration 400 / 1500: loss 2.166314\n",
      "iteration 500 / 1500: loss 2.174254\n",
      "iteration 600 / 1500: loss 2.188488\n",
      "iteration 700 / 1500: loss 2.143314\n",
      "iteration 800 / 1500: loss 2.130237\n",
      "iteration 900 / 1500: loss 2.122909\n",
      "iteration 1000 / 1500: loss 2.126368\n",
      "iteration 1100 / 1500: loss 2.125365\n",
      "iteration 1200 / 1500: loss 2.168156\n",
      "iteration 1300 / 1500: loss 2.170447\n",
      "iteration 1400 / 1500: loss 2.160549\n",
      "iteration 0 / 1500: loss 1467.407227\n",
      "iteration 100 / 1500: loss 102.200729\n",
      "iteration 200 / 1500: loss 8.973753\n",
      "iteration 300 / 1500: loss 2.591295\n",
      "iteration 400 / 1500: loss 2.197953\n",
      "iteration 500 / 1500: loss 2.098656\n",
      "iteration 600 / 1500: loss 2.159752\n",
      "iteration 700 / 1500: loss 2.121879\n",
      "iteration 800 / 1500: loss 2.118577\n",
      "iteration 900 / 1500: loss 2.126414\n",
      "iteration 1000 / 1500: loss 2.139620\n",
      "iteration 1100 / 1500: loss 2.112202\n",
      "iteration 1200 / 1500: loss 2.111802\n",
      "iteration 1300 / 1500: loss 2.171728\n",
      "iteration 1400 / 1500: loss 2.190986\n",
      "iteration 0 / 1500: loss 772.714582\n",
      "iteration 100 / 1500: loss 127.421626\n",
      "iteration 200 / 1500: loss 22.595886\n",
      "iteration 300 / 1500: loss 5.405502\n",
      "iteration 400 / 1500: loss 2.611846\n",
      "iteration 500 / 1500: loss 2.118550\n",
      "iteration 600 / 1500: loss 2.052258\n",
      "iteration 700 / 1500: loss 2.123781\n",
      "iteration 800 / 1500: loss 2.124497\n",
      "iteration 900 / 1500: loss 2.054070\n",
      "iteration 1000 / 1500: loss 2.073725\n",
      "iteration 1100 / 1500: loss 2.120388\n",
      "iteration 1200 / 1500: loss 2.117208\n",
      "iteration 1300 / 1500: loss 2.074921\n",
      "iteration 1400 / 1500: loss 2.051815\n",
      "iteration 0 / 1500: loss 857.246124\n",
      "iteration 100 / 1500: loss 118.283154\n",
      "iteration 200 / 1500: loss 17.884577\n",
      "iteration 300 / 1500: loss 4.212417\n",
      "iteration 400 / 1500: loss 2.382844\n",
      "iteration 500 / 1500: loss 2.097993\n",
      "iteration 600 / 1500: loss 2.064492\n",
      "iteration 700 / 1500: loss 2.079612\n",
      "iteration 800 / 1500: loss 2.087824\n",
      "iteration 900 / 1500: loss 2.103999\n",
      "iteration 1000 / 1500: loss 2.131096\n",
      "iteration 1100 / 1500: loss 2.130342\n",
      "iteration 1200 / 1500: loss 2.120900\n",
      "iteration 1300 / 1500: loss 2.121500\n",
      "iteration 1400 / 1500: loss 2.099398\n",
      "iteration 0 / 1500: loss 929.496042\n",
      "iteration 100 / 1500: loss 107.160736\n",
      "iteration 200 / 1500: loss 14.051610\n",
      "iteration 300 / 1500: loss 3.428688\n",
      "iteration 400 / 1500: loss 2.217345\n",
      "iteration 500 / 1500: loss 2.063670\n",
      "iteration 600 / 1500: loss 2.089584\n",
      "iteration 700 / 1500: loss 2.026460\n",
      "iteration 800 / 1500: loss 2.112669\n",
      "iteration 900 / 1500: loss 2.115098\n",
      "iteration 1000 / 1500: loss 2.063579\n",
      "iteration 1100 / 1500: loss 2.085391\n",
      "iteration 1200 / 1500: loss 2.100177\n",
      "iteration 1300 / 1500: loss 2.109824\n",
      "iteration 1400 / 1500: loss 2.038366\n",
      "iteration 0 / 1500: loss 1004.362992\n",
      "iteration 100 / 1500: loss 96.771749\n",
      "iteration 200 / 1500: loss 11.071730\n",
      "iteration 300 / 1500: loss 2.924113\n",
      "iteration 400 / 1500: loss 2.130081\n",
      "iteration 500 / 1500: loss 2.161926\n",
      "iteration 600 / 1500: loss 2.045782\n",
      "iteration 700 / 1500: loss 2.098439\n",
      "iteration 800 / 1500: loss 2.075430\n",
      "iteration 900 / 1500: loss 2.148812\n",
      "iteration 1000 / 1500: loss 2.073775\n",
      "iteration 1100 / 1500: loss 2.165261\n",
      "iteration 1200 / 1500: loss 2.101942\n",
      "iteration 1300 / 1500: loss 2.156815\n",
      "iteration 1400 / 1500: loss 2.187179\n",
      "iteration 0 / 1500: loss 1095.217453\n",
      "iteration 100 / 1500: loss 88.002634\n",
      "iteration 200 / 1500: loss 8.880869\n",
      "iteration 300 / 1500: loss 2.642267\n",
      "iteration 400 / 1500: loss 2.100022\n",
      "iteration 500 / 1500: loss 2.169900\n",
      "iteration 600 / 1500: loss 2.103562\n",
      "iteration 700 / 1500: loss 2.090340\n",
      "iteration 800 / 1500: loss 2.095666\n",
      "iteration 900 / 1500: loss 2.136464\n",
      "iteration 1000 / 1500: loss 2.174649\n",
      "iteration 1100 / 1500: loss 2.113818\n",
      "iteration 1200 / 1500: loss 2.086622\n",
      "iteration 1300 / 1500: loss 2.123477\n",
      "iteration 1400 / 1500: loss 2.066980\n",
      "iteration 0 / 1500: loss 1148.221850\n",
      "iteration 100 / 1500: loss 77.169168\n",
      "iteration 200 / 1500: loss 7.043923\n",
      "iteration 300 / 1500: loss 2.482338\n",
      "iteration 400 / 1500: loss 2.154243\n",
      "iteration 500 / 1500: loss 2.050405\n",
      "iteration 600 / 1500: loss 2.159800\n",
      "iteration 700 / 1500: loss 2.093141\n",
      "iteration 800 / 1500: loss 2.073755\n",
      "iteration 900 / 1500: loss 2.129637\n",
      "iteration 1000 / 1500: loss 2.133850\n",
      "iteration 1100 / 1500: loss 2.180615\n",
      "iteration 1200 / 1500: loss 2.150548\n",
      "iteration 1300 / 1500: loss 2.202499\n",
      "iteration 1400 / 1500: loss 2.100484\n",
      "iteration 0 / 1500: loss 1216.128045\n",
      "iteration 100 / 1500: loss 68.635113\n",
      "iteration 200 / 1500: loss 5.770816\n",
      "iteration 300 / 1500: loss 2.305337\n",
      "iteration 400 / 1500: loss 2.134079\n",
      "iteration 500 / 1500: loss 2.137023\n",
      "iteration 600 / 1500: loss 2.094819\n",
      "iteration 700 / 1500: loss 2.113317\n",
      "iteration 800 / 1500: loss 2.156991\n",
      "iteration 900 / 1500: loss 2.053600\n",
      "iteration 1000 / 1500: loss 2.107008\n",
      "iteration 1100 / 1500: loss 2.178580\n",
      "iteration 1200 / 1500: loss 2.109244\n",
      "iteration 1300 / 1500: loss 2.124800\n",
      "iteration 1400 / 1500: loss 2.141115\n",
      "iteration 0 / 1500: loss 1315.634190\n",
      "iteration 100 / 1500: loss 61.886322\n",
      "iteration 200 / 1500: loss 4.850060\n",
      "iteration 300 / 1500: loss 2.237795\n",
      "iteration 400 / 1500: loss 2.128059\n",
      "iteration 500 / 1500: loss 2.150123\n",
      "iteration 600 / 1500: loss 2.116228\n",
      "iteration 700 / 1500: loss 2.168060\n",
      "iteration 800 / 1500: loss 2.109242\n",
      "iteration 900 / 1500: loss 2.105729\n",
      "iteration 1000 / 1500: loss 2.147194\n",
      "iteration 1100 / 1500: loss 2.083061\n",
      "iteration 1200 / 1500: loss 2.151134\n",
      "iteration 1300 / 1500: loss 2.182921\n",
      "iteration 1400 / 1500: loss 2.139181\n",
      "iteration 0 / 1500: loss 1383.846594\n",
      "iteration 100 / 1500: loss 54.370292\n",
      "iteration 200 / 1500: loss 4.140869\n",
      "iteration 300 / 1500: loss 2.271298\n",
      "iteration 400 / 1500: loss 2.147084\n",
      "iteration 500 / 1500: loss 2.181727\n",
      "iteration 600 / 1500: loss 2.145831\n",
      "iteration 700 / 1500: loss 2.084488\n",
      "iteration 800 / 1500: loss 2.057115\n",
      "iteration 900 / 1500: loss 2.153495\n",
      "iteration 1000 / 1500: loss 2.169825\n",
      "iteration 1100 / 1500: loss 2.175800\n",
      "iteration 1200 / 1500: loss 2.146557\n",
      "iteration 1300 / 1500: loss 2.177797\n",
      "iteration 1400 / 1500: loss 2.107211\n",
      "iteration 0 / 1500: loss 1496.881893\n",
      "iteration 100 / 1500: loss 49.226133\n",
      "iteration 200 / 1500: loss 3.672359\n",
      "iteration 300 / 1500: loss 2.151128\n",
      "iteration 400 / 1500: loss 2.156364\n",
      "iteration 500 / 1500: loss 2.152196\n",
      "iteration 600 / 1500: loss 2.122085\n",
      "iteration 700 / 1500: loss 2.144062\n",
      "iteration 800 / 1500: loss 2.111563\n",
      "iteration 900 / 1500: loss 2.144058\n",
      "iteration 1000 / 1500: loss 2.138590\n",
      "iteration 1100 / 1500: loss 2.099722\n",
      "iteration 1200 / 1500: loss 2.194238\n",
      "iteration 1300 / 1500: loss 2.112097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.158330\n",
      "iteration 0 / 1500: loss 776.414745\n",
      "iteration 100 / 1500: loss 86.227037\n",
      "iteration 200 / 1500: loss 11.217114\n",
      "iteration 300 / 1500: loss 3.053039\n",
      "iteration 400 / 1500: loss 2.205063\n",
      "iteration 500 / 1500: loss 2.090829\n",
      "iteration 600 / 1500: loss 2.118144\n",
      "iteration 700 / 1500: loss 2.079934\n",
      "iteration 800 / 1500: loss 2.069198\n",
      "iteration 900 / 1500: loss 2.103460\n",
      "iteration 1000 / 1500: loss 2.132063\n",
      "iteration 1100 / 1500: loss 2.105226\n",
      "iteration 1200 / 1500: loss 2.110385\n",
      "iteration 1300 / 1500: loss 2.054653\n",
      "iteration 1400 / 1500: loss 2.053478\n",
      "iteration 0 / 1500: loss 861.166576\n",
      "iteration 100 / 1500: loss 76.709377\n",
      "iteration 200 / 1500: loss 8.589116\n",
      "iteration 300 / 1500: loss 2.729283\n",
      "iteration 400 / 1500: loss 2.137122\n",
      "iteration 500 / 1500: loss 2.096911\n",
      "iteration 600 / 1500: loss 2.111610\n",
      "iteration 700 / 1500: loss 2.119080\n",
      "iteration 800 / 1500: loss 2.101821\n",
      "iteration 900 / 1500: loss 2.104642\n",
      "iteration 1000 / 1500: loss 2.136846\n",
      "iteration 1100 / 1500: loss 2.071041\n",
      "iteration 1200 / 1500: loss 2.146601\n",
      "iteration 1300 / 1500: loss 2.106439\n",
      "iteration 1400 / 1500: loss 2.163084\n",
      "iteration 0 / 1500: loss 917.255642\n",
      "iteration 100 / 1500: loss 65.861939\n",
      "iteration 200 / 1500: loss 6.565066\n",
      "iteration 300 / 1500: loss 2.391093\n",
      "iteration 400 / 1500: loss 2.167878\n",
      "iteration 500 / 1500: loss 2.145091\n",
      "iteration 600 / 1500: loss 2.144504\n",
      "iteration 700 / 1500: loss 2.078109\n",
      "iteration 800 / 1500: loss 2.058378\n",
      "iteration 900 / 1500: loss 2.047179\n",
      "iteration 1000 / 1500: loss 2.081275\n",
      "iteration 1100 / 1500: loss 2.058862\n",
      "iteration 1200 / 1500: loss 2.153057\n",
      "iteration 1300 / 1500: loss 2.086900\n",
      "iteration 1400 / 1500: loss 2.099170\n",
      "iteration 0 / 1500: loss 1003.807966\n",
      "iteration 100 / 1500: loss 57.907290\n",
      "iteration 200 / 1500: loss 5.219935\n",
      "iteration 300 / 1500: loss 2.328845\n",
      "iteration 400 / 1500: loss 2.117938\n",
      "iteration 500 / 1500: loss 2.126546\n",
      "iteration 600 / 1500: loss 2.125742\n",
      "iteration 700 / 1500: loss 2.137480\n",
      "iteration 800 / 1500: loss 2.100392\n",
      "iteration 900 / 1500: loss 2.103350\n",
      "iteration 1000 / 1500: loss 2.121387\n",
      "iteration 1100 / 1500: loss 2.122827\n",
      "iteration 1200 / 1500: loss 2.138794\n",
      "iteration 1300 / 1500: loss 2.125788\n",
      "iteration 1400 / 1500: loss 2.101749\n",
      "iteration 0 / 1500: loss 1093.391100\n",
      "iteration 100 / 1500: loss 50.741152\n",
      "iteration 200 / 1500: loss 4.286392\n",
      "iteration 300 / 1500: loss 2.270072\n",
      "iteration 400 / 1500: loss 2.132649\n",
      "iteration 500 / 1500: loss 2.145337\n",
      "iteration 600 / 1500: loss 2.118508\n",
      "iteration 700 / 1500: loss 2.107974\n",
      "iteration 800 / 1500: loss 2.116260\n",
      "iteration 900 / 1500: loss 2.072456\n",
      "iteration 1000 / 1500: loss 2.136973\n",
      "iteration 1100 / 1500: loss 2.133170\n",
      "iteration 1200 / 1500: loss 2.092984\n",
      "iteration 1300 / 1500: loss 2.130866\n",
      "iteration 1400 / 1500: loss 2.145042\n",
      "iteration 0 / 1500: loss 1147.437446\n",
      "iteration 100 / 1500: loss 42.870131\n",
      "iteration 200 / 1500: loss 3.593726\n",
      "iteration 300 / 1500: loss 2.147183\n",
      "iteration 400 / 1500: loss 2.093266\n",
      "iteration 500 / 1500: loss 2.171858\n",
      "iteration 600 / 1500: loss 2.124249\n",
      "iteration 700 / 1500: loss 2.112713\n",
      "iteration 800 / 1500: loss 2.112838\n",
      "iteration 900 / 1500: loss 2.096911\n",
      "iteration 1000 / 1500: loss 2.173925\n",
      "iteration 1100 / 1500: loss 2.136480\n",
      "iteration 1200 / 1500: loss 2.146747\n",
      "iteration 1300 / 1500: loss 2.141096\n",
      "iteration 1400 / 1500: loss 2.120949\n",
      "iteration 0 / 1500: loss 1245.078921\n",
      "iteration 100 / 1500: loss 37.508587\n",
      "iteration 200 / 1500: loss 3.146924\n",
      "iteration 300 / 1500: loss 2.161666\n",
      "iteration 400 / 1500: loss 2.126509\n",
      "iteration 500 / 1500: loss 2.156721\n",
      "iteration 600 / 1500: loss 2.126715\n",
      "iteration 700 / 1500: loss 2.007860\n",
      "iteration 800 / 1500: loss 2.112955\n",
      "iteration 900 / 1500: loss 2.127978\n",
      "iteration 1000 / 1500: loss 2.137726\n",
      "iteration 1100 / 1500: loss 2.151005\n",
      "iteration 1200 / 1500: loss 2.189746\n",
      "iteration 1300 / 1500: loss 2.076245\n",
      "iteration 1400 / 1500: loss 2.129915\n",
      "iteration 0 / 1500: loss 1306.011991\n",
      "iteration 100 / 1500: loss 31.829291\n",
      "iteration 200 / 1500: loss 2.796384\n",
      "iteration 300 / 1500: loss 2.192090\n",
      "iteration 400 / 1500: loss 2.152480\n",
      "iteration 500 / 1500: loss 2.103806\n",
      "iteration 600 / 1500: loss 2.149819\n",
      "iteration 700 / 1500: loss 2.128296\n",
      "iteration 800 / 1500: loss 2.149619\n",
      "iteration 900 / 1500: loss 2.049098\n",
      "iteration 1000 / 1500: loss 2.146895\n",
      "iteration 1100 / 1500: loss 2.161443\n",
      "iteration 1200 / 1500: loss 2.086250\n",
      "iteration 1300 / 1500: loss 2.133384\n",
      "iteration 1400 / 1500: loss 2.155768\n",
      "iteration 0 / 1500: loss 1389.354220\n",
      "iteration 100 / 1500: loss 27.337209\n",
      "iteration 200 / 1500: loss 2.595585\n",
      "iteration 300 / 1500: loss 2.150549\n",
      "iteration 400 / 1500: loss 2.084861\n",
      "iteration 500 / 1500: loss 2.178166\n",
      "iteration 600 / 1500: loss 2.156411\n",
      "iteration 700 / 1500: loss 2.173479\n",
      "iteration 800 / 1500: loss 2.130122\n",
      "iteration 900 / 1500: loss 2.145501\n",
      "iteration 1000 / 1500: loss 2.142618\n",
      "iteration 1100 / 1500: loss 2.105912\n",
      "iteration 1200 / 1500: loss 2.112383\n",
      "iteration 1300 / 1500: loss 2.158794\n",
      "iteration 1400 / 1500: loss 2.148504\n",
      "iteration 0 / 1500: loss 1462.693953\n",
      "iteration 100 / 1500: loss 23.336599\n",
      "iteration 200 / 1500: loss 2.455437\n",
      "iteration 300 / 1500: loss 2.133743\n",
      "iteration 400 / 1500: loss 2.175132\n",
      "iteration 500 / 1500: loss 2.165209\n",
      "iteration 600 / 1500: loss 2.127882\n",
      "iteration 700 / 1500: loss 2.174166\n",
      "iteration 800 / 1500: loss 2.124989\n",
      "iteration 900 / 1500: loss 2.139324\n",
      "iteration 1000 / 1500: loss 2.102010\n",
      "iteration 1100 / 1500: loss 2.133028\n",
      "iteration 1200 / 1500: loss 2.153674\n",
      "iteration 1300 / 1500: loss 2.149109\n",
      "iteration 1400 / 1500: loss 2.100254\n",
      "iteration 0 / 1500: loss 773.303076\n",
      "iteration 100 / 1500: loss 57.927672\n",
      "iteration 200 / 1500: loss 6.105073\n",
      "iteration 300 / 1500: loss 2.423111\n",
      "iteration 400 / 1500: loss 2.124019\n",
      "iteration 500 / 1500: loss 2.062076\n",
      "iteration 600 / 1500: loss 2.058421\n",
      "iteration 700 / 1500: loss 2.112860\n",
      "iteration 800 / 1500: loss 2.076213\n",
      "iteration 900 / 1500: loss 2.170552\n",
      "iteration 1000 / 1500: loss 2.057865\n",
      "iteration 1100 / 1500: loss 2.123232\n",
      "iteration 1200 / 1500: loss 2.100421\n",
      "iteration 1300 / 1500: loss 2.069074\n",
      "iteration 1400 / 1500: loss 2.077855\n",
      "iteration 0 / 1500: loss 842.947789\n",
      "iteration 100 / 1500: loss 48.739265\n",
      "iteration 200 / 1500: loss 4.744378\n",
      "iteration 300 / 1500: loss 2.252405\n",
      "iteration 400 / 1500: loss 2.102948\n",
      "iteration 500 / 1500: loss 2.131779\n",
      "iteration 600 / 1500: loss 2.061771\n",
      "iteration 700 / 1500: loss 2.056039\n",
      "iteration 800 / 1500: loss 2.074136\n",
      "iteration 900 / 1500: loss 2.115886\n",
      "iteration 1000 / 1500: loss 2.147017\n",
      "iteration 1100 / 1500: loss 2.182908\n",
      "iteration 1200 / 1500: loss 2.123732\n",
      "iteration 1300 / 1500: loss 2.086069\n",
      "iteration 1400 / 1500: loss 2.099465\n",
      "iteration 0 / 1500: loss 923.508632\n",
      "iteration 100 / 1500: loss 41.504843\n",
      "iteration 200 / 1500: loss 3.805867\n",
      "iteration 300 / 1500: loss 2.145939\n",
      "iteration 400 / 1500: loss 2.149120\n",
      "iteration 500 / 1500: loss 2.129862\n",
      "iteration 600 / 1500: loss 2.106421\n",
      "iteration 700 / 1500: loss 2.103718\n",
      "iteration 800 / 1500: loss 2.077039\n",
      "iteration 900 / 1500: loss 2.068567\n",
      "iteration 1000 / 1500: loss 2.117652\n",
      "iteration 1100 / 1500: loss 2.069422\n",
      "iteration 1200 / 1500: loss 2.084585\n",
      "iteration 1300 / 1500: loss 2.059835\n",
      "iteration 1400 / 1500: loss 2.088663\n",
      "iteration 0 / 1500: loss 1018.788530\n",
      "iteration 100 / 1500: loss 35.398294\n",
      "iteration 200 / 1500: loss 3.178352\n",
      "iteration 300 / 1500: loss 2.136413\n",
      "iteration 400 / 1500: loss 2.115869\n",
      "iteration 500 / 1500: loss 2.114376\n",
      "iteration 600 / 1500: loss 2.107019\n",
      "iteration 700 / 1500: loss 2.108925\n",
      "iteration 800 / 1500: loss 2.091546\n",
      "iteration 900 / 1500: loss 2.114347\n",
      "iteration 1000 / 1500: loss 2.157455\n",
      "iteration 1100 / 1500: loss 2.132105\n",
      "iteration 1200 / 1500: loss 2.083579\n",
      "iteration 1300 / 1500: loss 2.110502\n",
      "iteration 1400 / 1500: loss 2.128116\n",
      "iteration 0 / 1500: loss 1079.033951\n",
      "iteration 100 / 1500: loss 29.219838\n",
      "iteration 200 / 1500: loss 2.827910\n",
      "iteration 300 / 1500: loss 2.114152\n",
      "iteration 400 / 1500: loss 2.094788\n",
      "iteration 500 / 1500: loss 2.114527\n",
      "iteration 600 / 1500: loss 2.165845\n",
      "iteration 700 / 1500: loss 2.124893\n",
      "iteration 800 / 1500: loss 2.107064\n",
      "iteration 900 / 1500: loss 2.122627\n",
      "iteration 1000 / 1500: loss 2.141179\n",
      "iteration 1100 / 1500: loss 2.116860\n",
      "iteration 1200 / 1500: loss 2.104594\n",
      "iteration 1300 / 1500: loss 2.176713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.159053\n",
      "iteration 0 / 1500: loss 1158.508906\n",
      "iteration 100 / 1500: loss 24.388501\n",
      "iteration 200 / 1500: loss 2.592440\n",
      "iteration 300 / 1500: loss 2.155459\n",
      "iteration 400 / 1500: loss 2.164632\n",
      "iteration 500 / 1500: loss 2.121752\n",
      "iteration 600 / 1500: loss 2.050452\n",
      "iteration 700 / 1500: loss 2.130625\n",
      "iteration 800 / 1500: loss 2.151687\n",
      "iteration 900 / 1500: loss 2.109564\n",
      "iteration 1000 / 1500: loss 2.106984\n",
      "iteration 1100 / 1500: loss 2.123734\n",
      "iteration 1200 / 1500: loss 2.048825\n",
      "iteration 1300 / 1500: loss 2.186413\n",
      "iteration 1400 / 1500: loss 2.081473\n",
      "iteration 0 / 1500: loss 1243.199551\n",
      "iteration 100 / 1500: loss 20.466235\n",
      "iteration 200 / 1500: loss 2.402629\n",
      "iteration 300 / 1500: loss 2.117772\n",
      "iteration 400 / 1500: loss 2.087331\n",
      "iteration 500 / 1500: loss 2.169810\n",
      "iteration 600 / 1500: loss 2.066377\n",
      "iteration 700 / 1500: loss 2.098622\n",
      "iteration 800 / 1500: loss 2.164067\n",
      "iteration 900 / 1500: loss 2.162010\n",
      "iteration 1000 / 1500: loss 2.155294\n",
      "iteration 1100 / 1500: loss 2.110600\n",
      "iteration 1200 / 1500: loss 2.129752\n",
      "iteration 1300 / 1500: loss 2.106613\n",
      "iteration 1400 / 1500: loss 2.127144\n",
      "iteration 0 / 1500: loss 1300.264021\n",
      "iteration 100 / 1500: loss 16.837016\n",
      "iteration 200 / 1500: loss 2.314904\n",
      "iteration 300 / 1500: loss 2.160651\n",
      "iteration 400 / 1500: loss 2.162195\n",
      "iteration 500 / 1500: loss 2.164334\n",
      "iteration 600 / 1500: loss 2.163959\n",
      "iteration 700 / 1500: loss 2.104963\n",
      "iteration 800 / 1500: loss 2.145960\n",
      "iteration 900 / 1500: loss 2.183957\n",
      "iteration 1000 / 1500: loss 2.119658\n",
      "iteration 1100 / 1500: loss 2.080230\n",
      "iteration 1200 / 1500: loss 2.137502\n",
      "iteration 1300 / 1500: loss 2.133393\n",
      "iteration 1400 / 1500: loss 2.142803\n",
      "iteration 0 / 1500: loss 1401.385760\n",
      "iteration 100 / 1500: loss 14.294738\n",
      "iteration 200 / 1500: loss 2.261392\n",
      "iteration 300 / 1500: loss 2.142090\n",
      "iteration 400 / 1500: loss 2.128624\n",
      "iteration 500 / 1500: loss 2.110958\n",
      "iteration 600 / 1500: loss 2.168044\n",
      "iteration 700 / 1500: loss 2.185126\n",
      "iteration 800 / 1500: loss 2.119111\n",
      "iteration 900 / 1500: loss 2.108784\n",
      "iteration 1000 / 1500: loss 2.135265\n",
      "iteration 1100 / 1500: loss 2.170727\n",
      "iteration 1200 / 1500: loss 2.170253\n",
      "iteration 1300 / 1500: loss 2.119837\n",
      "iteration 1400 / 1500: loss 2.117491\n",
      "iteration 0 / 1500: loss 1478.615282\n",
      "iteration 100 / 1500: loss 11.954623\n",
      "iteration 200 / 1500: loss 2.200685\n",
      "iteration 300 / 1500: loss 2.174087\n",
      "iteration 400 / 1500: loss 2.166600\n",
      "iteration 500 / 1500: loss 2.133105\n",
      "iteration 600 / 1500: loss 2.107309\n",
      "iteration 700 / 1500: loss 2.119282\n",
      "iteration 800 / 1500: loss 2.129130\n",
      "iteration 900 / 1500: loss 2.133769\n",
      "iteration 1000 / 1500: loss 2.118427\n",
      "iteration 1100 / 1500: loss 2.174282\n",
      "iteration 1200 / 1500: loss 2.133722\n",
      "iteration 1300 / 1500: loss 2.152578\n",
      "iteration 1400 / 1500: loss 2.173520\n",
      "iteration 0 / 1500: loss 770.214873\n",
      "iteration 100 / 1500: loss 39.070165\n",
      "iteration 200 / 1500: loss 3.890908\n",
      "iteration 300 / 1500: loss 2.153692\n",
      "iteration 400 / 1500: loss 2.063963\n",
      "iteration 500 / 1500: loss 2.062344\n",
      "iteration 600 / 1500: loss 2.071285\n",
      "iteration 700 / 1500: loss 2.149860\n",
      "iteration 800 / 1500: loss 2.109801\n",
      "iteration 900 / 1500: loss 2.071731\n",
      "iteration 1000 / 1500: loss 2.071221\n",
      "iteration 1100 / 1500: loss 2.063524\n",
      "iteration 1200 / 1500: loss 2.125916\n",
      "iteration 1300 / 1500: loss 2.078699\n",
      "iteration 1400 / 1500: loss 2.128551\n",
      "iteration 0 / 1500: loss 875.909343\n",
      "iteration 100 / 1500: loss 33.121882\n",
      "iteration 200 / 1500: loss 3.169382\n",
      "iteration 300 / 1500: loss 2.090602\n",
      "iteration 400 / 1500: loss 2.102585\n",
      "iteration 500 / 1500: loss 2.120177\n",
      "iteration 600 / 1500: loss 2.118546\n",
      "iteration 700 / 1500: loss 2.112551\n",
      "iteration 800 / 1500: loss 2.119832\n",
      "iteration 900 / 1500: loss 2.102661\n",
      "iteration 1000 / 1500: loss 2.054754\n",
      "iteration 1100 / 1500: loss 2.124972\n",
      "iteration 1200 / 1500: loss 2.045574\n",
      "iteration 1300 / 1500: loss 2.049595\n",
      "iteration 1400 / 1500: loss 2.075161\n",
      "iteration 0 / 1500: loss 938.307055\n",
      "iteration 100 / 1500: loss 26.664827\n",
      "iteration 200 / 1500: loss 2.756426\n",
      "iteration 300 / 1500: loss 2.061604\n",
      "iteration 400 / 1500: loss 2.074025\n",
      "iteration 500 / 1500: loss 2.152571\n",
      "iteration 600 / 1500: loss 2.130405\n",
      "iteration 700 / 1500: loss 2.122095\n",
      "iteration 800 / 1500: loss 2.058755\n",
      "iteration 900 / 1500: loss 2.119875\n",
      "iteration 1000 / 1500: loss 2.089502\n",
      "iteration 1100 / 1500: loss 2.067463\n",
      "iteration 1200 / 1500: loss 2.082294\n",
      "iteration 1300 / 1500: loss 2.151957\n",
      "iteration 1400 / 1500: loss 2.073633\n",
      "iteration 0 / 1500: loss 1017.082814\n",
      "iteration 100 / 1500: loss 21.683194\n",
      "iteration 200 / 1500: loss 2.518594\n",
      "iteration 300 / 1500: loss 2.066357\n",
      "iteration 400 / 1500: loss 2.162150\n",
      "iteration 500 / 1500: loss 2.084956\n",
      "iteration 600 / 1500: loss 2.073560\n",
      "iteration 700 / 1500: loss 2.126477\n",
      "iteration 800 / 1500: loss 2.092315\n",
      "iteration 900 / 1500: loss 2.152719\n",
      "iteration 1000 / 1500: loss 2.086553\n",
      "iteration 1100 / 1500: loss 2.073790\n",
      "iteration 1200 / 1500: loss 2.085022\n",
      "iteration 1300 / 1500: loss 2.133425\n",
      "iteration 1400 / 1500: loss 2.071111\n",
      "iteration 0 / 1500: loss 1088.386787\n",
      "iteration 100 / 1500: loss 17.570781\n",
      "iteration 200 / 1500: loss 2.320036\n",
      "iteration 300 / 1500: loss 2.114628\n",
      "iteration 400 / 1500: loss 2.146604\n",
      "iteration 500 / 1500: loss 2.160633\n",
      "iteration 600 / 1500: loss 2.148221\n",
      "iteration 700 / 1500: loss 2.115151\n",
      "iteration 800 / 1500: loss 2.122397\n",
      "iteration 900 / 1500: loss 2.078600\n",
      "iteration 1000 / 1500: loss 2.106278\n",
      "iteration 1100 / 1500: loss 2.130707\n",
      "iteration 1200 / 1500: loss 2.126278\n",
      "iteration 1300 / 1500: loss 2.115318\n",
      "iteration 1400 / 1500: loss 2.125269\n",
      "iteration 0 / 1500: loss 1169.155416\n",
      "iteration 100 / 1500: loss 14.300574\n",
      "iteration 200 / 1500: loss 2.211397\n",
      "iteration 300 / 1500: loss 2.145543\n",
      "iteration 400 / 1500: loss 2.149416\n",
      "iteration 500 / 1500: loss 2.112501\n",
      "iteration 600 / 1500: loss 2.140345\n",
      "iteration 700 / 1500: loss 2.084686\n",
      "iteration 800 / 1500: loss 2.144065\n",
      "iteration 900 / 1500: loss 2.139513\n",
      "iteration 1000 / 1500: loss 2.162944\n",
      "iteration 1100 / 1500: loss 2.122732\n",
      "iteration 1200 / 1500: loss 2.123345\n",
      "iteration 1300 / 1500: loss 2.133322\n",
      "iteration 1400 / 1500: loss 2.137725\n",
      "iteration 0 / 1500: loss 1231.692001\n",
      "iteration 100 / 1500: loss 11.567647\n",
      "iteration 200 / 1500: loss 2.219242\n",
      "iteration 300 / 1500: loss 2.098646\n",
      "iteration 400 / 1500: loss 2.176020\n",
      "iteration 500 / 1500: loss 2.171072\n",
      "iteration 600 / 1500: loss 2.130475\n",
      "iteration 700 / 1500: loss 2.130607\n",
      "iteration 800 / 1500: loss 2.097010\n",
      "iteration 900 / 1500: loss 2.135879\n",
      "iteration 1000 / 1500: loss 2.132346\n",
      "iteration 1100 / 1500: loss 2.172508\n",
      "iteration 1200 / 1500: loss 2.141011\n",
      "iteration 1300 / 1500: loss 2.123896\n",
      "iteration 1400 / 1500: loss 2.168324\n",
      "iteration 0 / 1500: loss 1301.689199\n",
      "iteration 100 / 1500: loss 9.471497\n",
      "iteration 200 / 1500: loss 2.177619\n",
      "iteration 300 / 1500: loss 2.130992\n",
      "iteration 400 / 1500: loss 2.105043\n",
      "iteration 500 / 1500: loss 2.112056\n",
      "iteration 600 / 1500: loss 2.132960\n",
      "iteration 700 / 1500: loss 2.081258\n",
      "iteration 800 / 1500: loss 2.198167\n",
      "iteration 900 / 1500: loss 2.075924\n",
      "iteration 1000 / 1500: loss 2.076627\n",
      "iteration 1100 / 1500: loss 2.141320\n",
      "iteration 1200 / 1500: loss 2.145269\n",
      "iteration 1300 / 1500: loss 2.108422\n",
      "iteration 1400 / 1500: loss 2.167259\n",
      "iteration 0 / 1500: loss 1385.232470\n",
      "iteration 100 / 1500: loss 7.874447\n",
      "iteration 200 / 1500: loss 2.152199\n",
      "iteration 300 / 1500: loss 2.136934\n",
      "iteration 400 / 1500: loss 2.167791\n",
      "iteration 500 / 1500: loss 2.141529\n",
      "iteration 600 / 1500: loss 2.132580\n",
      "iteration 700 / 1500: loss 2.096124\n",
      "iteration 800 / 1500: loss 2.107191\n",
      "iteration 900 / 1500: loss 2.146603\n",
      "iteration 1000 / 1500: loss 2.175270\n",
      "iteration 1100 / 1500: loss 2.163620\n",
      "iteration 1200 / 1500: loss 2.155182\n",
      "iteration 1300 / 1500: loss 2.200878\n",
      "iteration 1400 / 1500: loss 2.103901\n",
      "iteration 0 / 1500: loss 1469.476684\n",
      "iteration 100 / 1500: loss 6.669373\n",
      "iteration 200 / 1500: loss 2.154287\n",
      "iteration 300 / 1500: loss 2.115190\n",
      "iteration 400 / 1500: loss 2.184927\n",
      "iteration 500 / 1500: loss 2.101102\n",
      "iteration 600 / 1500: loss 2.131402\n",
      "iteration 700 / 1500: loss 2.117501\n",
      "iteration 800 / 1500: loss 2.140470\n",
      "iteration 900 / 1500: loss 2.154342\n",
      "iteration 1000 / 1500: loss 2.206130\n",
      "iteration 1100 / 1500: loss 2.100456\n",
      "iteration 1200 / 1500: loss 2.153285\n",
      "iteration 1300 / 1500: loss 2.207833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.132487\n",
      "iteration 0 / 1500: loss 780.834923\n",
      "iteration 100 / 1500: loss 27.086847\n",
      "iteration 200 / 1500: loss 2.936084\n",
      "iteration 300 / 1500: loss 2.158113\n",
      "iteration 400 / 1500: loss 2.130258\n",
      "iteration 500 / 1500: loss 2.078379\n",
      "iteration 600 / 1500: loss 2.058089\n",
      "iteration 700 / 1500: loss 2.038029\n",
      "iteration 800 / 1500: loss 2.105492\n",
      "iteration 900 / 1500: loss 2.119842\n",
      "iteration 1000 / 1500: loss 2.140061\n",
      "iteration 1100 / 1500: loss 2.101436\n",
      "iteration 1200 / 1500: loss 2.021261\n",
      "iteration 1300 / 1500: loss 2.085792\n",
      "iteration 1400 / 1500: loss 2.133538\n",
      "iteration 0 / 1500: loss 857.408035\n",
      "iteration 100 / 1500: loss 21.415741\n",
      "iteration 200 / 1500: loss 2.539725\n",
      "iteration 300 / 1500: loss 2.115418\n",
      "iteration 400 / 1500: loss 2.078211\n",
      "iteration 500 / 1500: loss 2.062418\n",
      "iteration 600 / 1500: loss 2.139356\n",
      "iteration 700 / 1500: loss 2.100701\n",
      "iteration 800 / 1500: loss 2.145167\n",
      "iteration 900 / 1500: loss 2.074699\n",
      "iteration 1000 / 1500: loss 2.193209\n",
      "iteration 1100 / 1500: loss 2.153520\n",
      "iteration 1200 / 1500: loss 2.105881\n",
      "iteration 1300 / 1500: loss 2.061351\n",
      "iteration 1400 / 1500: loss 2.069712\n",
      "iteration 0 / 1500: loss 916.279825\n",
      "iteration 100 / 1500: loss 16.791030\n",
      "iteration 200 / 1500: loss 2.380535\n",
      "iteration 300 / 1500: loss 2.131819\n",
      "iteration 400 / 1500: loss 2.098530\n",
      "iteration 500 / 1500: loss 2.121969\n",
      "iteration 600 / 1500: loss 2.132908\n",
      "iteration 700 / 1500: loss 2.122964\n",
      "iteration 800 / 1500: loss 2.115558\n",
      "iteration 900 / 1500: loss 2.174163\n",
      "iteration 1000 / 1500: loss 2.116150\n",
      "iteration 1100 / 1500: loss 2.093581\n",
      "iteration 1200 / 1500: loss 2.088680\n",
      "iteration 1300 / 1500: loss 2.084285\n",
      "iteration 1400 / 1500: loss 2.084398\n",
      "iteration 0 / 1500: loss 1010.738481\n",
      "iteration 100 / 1500: loss 13.523101\n",
      "iteration 200 / 1500: loss 2.263895\n",
      "iteration 300 / 1500: loss 2.127616\n",
      "iteration 400 / 1500: loss 2.133119\n",
      "iteration 500 / 1500: loss 2.115893\n",
      "iteration 600 / 1500: loss 2.096812\n",
      "iteration 700 / 1500: loss 2.112230\n",
      "iteration 800 / 1500: loss 2.153826\n",
      "iteration 900 / 1500: loss 2.139724\n",
      "iteration 1000 / 1500: loss 2.073943\n",
      "iteration 1100 / 1500: loss 2.132650\n",
      "iteration 1200 / 1500: loss 2.088146\n",
      "iteration 1300 / 1500: loss 2.065609\n",
      "iteration 1400 / 1500: loss 2.026178\n",
      "iteration 0 / 1500: loss 1090.379617\n",
      "iteration 100 / 1500: loss 10.827796\n",
      "iteration 200 / 1500: loss 2.190670\n",
      "iteration 300 / 1500: loss 2.154409\n",
      "iteration 400 / 1500: loss 2.106176\n",
      "iteration 500 / 1500: loss 2.090386\n",
      "iteration 600 / 1500: loss 2.091359\n",
      "iteration 700 / 1500: loss 2.122685\n",
      "iteration 800 / 1500: loss 2.107448\n",
      "iteration 900 / 1500: loss 2.087138\n",
      "iteration 1000 / 1500: loss 2.137076\n",
      "iteration 1100 / 1500: loss 2.073997\n",
      "iteration 1200 / 1500: loss 2.098972\n",
      "iteration 1300 / 1500: loss 2.125868\n",
      "iteration 1400 / 1500: loss 2.090950\n",
      "iteration 0 / 1500: loss 1142.487811\n",
      "iteration 100 / 1500: loss 8.545550\n",
      "iteration 200 / 1500: loss 2.127643\n",
      "iteration 300 / 1500: loss 2.142714\n",
      "iteration 400 / 1500: loss 2.122674\n",
      "iteration 500 / 1500: loss 2.139108\n",
      "iteration 600 / 1500: loss 2.155304\n",
      "iteration 700 / 1500: loss 2.124440\n",
      "iteration 800 / 1500: loss 2.109406\n",
      "iteration 900 / 1500: loss 2.073385\n",
      "iteration 1000 / 1500: loss 2.137519\n",
      "iteration 1100 / 1500: loss 2.093796\n",
      "iteration 1200 / 1500: loss 2.127245\n",
      "iteration 1300 / 1500: loss 2.084654\n",
      "iteration 1400 / 1500: loss 2.073438\n",
      "iteration 0 / 1500: loss 1241.855705\n",
      "iteration 100 / 1500: loss 7.044703\n",
      "iteration 200 / 1500: loss 2.137328\n",
      "iteration 300 / 1500: loss 2.124816\n",
      "iteration 400 / 1500: loss 2.131796\n",
      "iteration 500 / 1500: loss 2.156340\n",
      "iteration 600 / 1500: loss 2.153417\n",
      "iteration 700 / 1500: loss 2.097893\n",
      "iteration 800 / 1500: loss 2.192483\n",
      "iteration 900 / 1500: loss 2.130888\n",
      "iteration 1000 / 1500: loss 2.111846\n",
      "iteration 1100 / 1500: loss 2.117539\n",
      "iteration 1200 / 1500: loss 2.086257\n",
      "iteration 1300 / 1500: loss 2.137678\n",
      "iteration 1400 / 1500: loss 2.158142\n",
      "iteration 0 / 1500: loss 1308.388335\n",
      "iteration 100 / 1500: loss 5.782097\n",
      "iteration 200 / 1500: loss 2.151952\n",
      "iteration 300 / 1500: loss 2.146376\n",
      "iteration 400 / 1500: loss 2.112182\n",
      "iteration 500 / 1500: loss 2.166830\n",
      "iteration 600 / 1500: loss 2.127918\n",
      "iteration 700 / 1500: loss 2.158868\n",
      "iteration 800 / 1500: loss 2.153287\n",
      "iteration 900 / 1500: loss 2.110125\n",
      "iteration 1000 / 1500: loss 2.083349\n",
      "iteration 1100 / 1500: loss 2.202493\n",
      "iteration 1200 / 1500: loss 2.098237\n",
      "iteration 1300 / 1500: loss 2.092578\n",
      "iteration 1400 / 1500: loss 2.112669\n",
      "iteration 0 / 1500: loss 1388.112369\n",
      "iteration 100 / 1500: loss 4.918056\n",
      "iteration 200 / 1500: loss 2.197651\n",
      "iteration 300 / 1500: loss 2.101299\n",
      "iteration 400 / 1500: loss 2.142812\n",
      "iteration 500 / 1500: loss 2.142744\n",
      "iteration 600 / 1500: loss 2.169157\n",
      "iteration 700 / 1500: loss 2.090839\n",
      "iteration 800 / 1500: loss 2.136558\n",
      "iteration 900 / 1500: loss 2.116264\n",
      "iteration 1000 / 1500: loss 2.138415\n",
      "iteration 1100 / 1500: loss 2.142498\n",
      "iteration 1200 / 1500: loss 2.165935\n",
      "iteration 1300 / 1500: loss 2.155166\n",
      "iteration 1400 / 1500: loss 2.192293\n",
      "iteration 0 / 1500: loss 1464.909276\n",
      "iteration 100 / 1500: loss 4.158993\n",
      "iteration 200 / 1500: loss 2.150380\n",
      "iteration 300 / 1500: loss 2.163316\n",
      "iteration 400 / 1500: loss 2.113509\n",
      "iteration 500 / 1500: loss 2.101128\n",
      "iteration 600 / 1500: loss 2.160148\n",
      "iteration 700 / 1500: loss 2.189437\n",
      "iteration 800 / 1500: loss 2.122729\n",
      "iteration 900 / 1500: loss 2.088979\n",
      "iteration 1000 / 1500: loss 2.111422\n",
      "iteration 1100 / 1500: loss 2.162016\n",
      "iteration 1200 / 1500: loss 2.079002\n",
      "iteration 1300 / 1500: loss 2.116492\n",
      "iteration 1400 / 1500: loss 2.143543\n",
      "iteration 0 / 1500: loss 785.792697\n",
      "iteration 100 / 1500: loss 18.747275\n",
      "iteration 200 / 1500: loss 2.443823\n",
      "iteration 300 / 1500: loss 2.119309\n",
      "iteration 400 / 1500: loss 2.132447\n",
      "iteration 500 / 1500: loss 2.043234\n",
      "iteration 600 / 1500: loss 2.099765\n",
      "iteration 700 / 1500: loss 2.067238\n",
      "iteration 800 / 1500: loss 2.130506\n",
      "iteration 900 / 1500: loss 2.117232\n",
      "iteration 1000 / 1500: loss 2.075998\n",
      "iteration 1100 / 1500: loss 2.137271\n",
      "iteration 1200 / 1500: loss 2.043772\n",
      "iteration 1300 / 1500: loss 2.058510\n",
      "iteration 1400 / 1500: loss 2.038435\n",
      "iteration 0 / 1500: loss 857.326229\n",
      "iteration 100 / 1500: loss 14.479996\n",
      "iteration 200 / 1500: loss 2.293017\n",
      "iteration 300 / 1500: loss 2.132127\n",
      "iteration 400 / 1500: loss 2.105658\n",
      "iteration 500 / 1500: loss 2.150749\n",
      "iteration 600 / 1500: loss 2.118300\n",
      "iteration 700 / 1500: loss 2.095705\n",
      "iteration 800 / 1500: loss 2.055363\n",
      "iteration 900 / 1500: loss 2.078005\n",
      "iteration 1000 / 1500: loss 2.133105\n",
      "iteration 1100 / 1500: loss 2.100557\n",
      "iteration 1200 / 1500: loss 2.124565\n",
      "iteration 1300 / 1500: loss 2.092223\n",
      "iteration 1400 / 1500: loss 2.117106\n",
      "iteration 0 / 1500: loss 925.949696\n",
      "iteration 100 / 1500: loss 11.179635\n",
      "iteration 200 / 1500: loss 2.229764\n",
      "iteration 300 / 1500: loss 2.098989\n",
      "iteration 400 / 1500: loss 2.075192\n",
      "iteration 500 / 1500: loss 2.113181\n",
      "iteration 600 / 1500: loss 2.100514\n",
      "iteration 700 / 1500: loss 2.082557\n",
      "iteration 800 / 1500: loss 2.059587\n",
      "iteration 900 / 1500: loss 2.077854\n",
      "iteration 1000 / 1500: loss 2.142736\n",
      "iteration 1100 / 1500: loss 2.134737\n",
      "iteration 1200 / 1500: loss 2.106485\n",
      "iteration 1300 / 1500: loss 2.052445\n",
      "iteration 1400 / 1500: loss 2.092834\n",
      "iteration 0 / 1500: loss 998.803489\n",
      "iteration 100 / 1500: loss 8.679937\n",
      "iteration 200 / 1500: loss 2.175355\n",
      "iteration 300 / 1500: loss 2.150979\n",
      "iteration 400 / 1500: loss 2.050315\n",
      "iteration 500 / 1500: loss 2.108268\n",
      "iteration 600 / 1500: loss 2.116111\n",
      "iteration 700 / 1500: loss 2.097065\n",
      "iteration 800 / 1500: loss 2.129716\n",
      "iteration 900 / 1500: loss 2.143134\n",
      "iteration 1000 / 1500: loss 2.088453\n",
      "iteration 1100 / 1500: loss 2.132208\n",
      "iteration 1200 / 1500: loss 2.155116\n",
      "iteration 1300 / 1500: loss 2.104756\n",
      "iteration 1400 / 1500: loss 2.137468\n",
      "iteration 0 / 1500: loss 1090.311006\n",
      "iteration 100 / 1500: loss 7.016382\n",
      "iteration 200 / 1500: loss 2.137904\n",
      "iteration 300 / 1500: loss 2.050283\n",
      "iteration 400 / 1500: loss 2.062555\n",
      "iteration 500 / 1500: loss 2.124499\n",
      "iteration 600 / 1500: loss 2.067872\n",
      "iteration 700 / 1500: loss 2.120769\n",
      "iteration 800 / 1500: loss 2.128247\n",
      "iteration 900 / 1500: loss 2.120676\n",
      "iteration 1000 / 1500: loss 2.135400\n",
      "iteration 1100 / 1500: loss 2.096348\n",
      "iteration 1200 / 1500: loss 2.102040\n",
      "iteration 1300 / 1500: loss 2.102099\n",
      "iteration 1400 / 1500: loss 2.076633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 1157.512849\n",
      "iteration 100 / 1500: loss 5.671607\n",
      "iteration 200 / 1500: loss 2.129361\n",
      "iteration 300 / 1500: loss 2.166019\n",
      "iteration 400 / 1500: loss 2.115928\n",
      "iteration 500 / 1500: loss 2.162974\n",
      "iteration 600 / 1500: loss 2.116418\n",
      "iteration 700 / 1500: loss 2.124307\n",
      "iteration 800 / 1500: loss 2.121873\n",
      "iteration 900 / 1500: loss 2.150941\n",
      "iteration 1000 / 1500: loss 2.139988\n",
      "iteration 1100 / 1500: loss 2.106584\n",
      "iteration 1200 / 1500: loss 2.139887\n",
      "iteration 1300 / 1500: loss 2.115913\n",
      "iteration 1400 / 1500: loss 2.123200\n",
      "iteration 0 / 1500: loss 1254.295048\n",
      "iteration 100 / 1500: loss 4.735219\n",
      "iteration 200 / 1500: loss 2.160973\n",
      "iteration 300 / 1500: loss 2.136001\n",
      "iteration 400 / 1500: loss 2.159995\n",
      "iteration 500 / 1500: loss 2.130687\n",
      "iteration 600 / 1500: loss 2.108718\n",
      "iteration 700 / 1500: loss 2.115658\n",
      "iteration 800 / 1500: loss 2.066012\n",
      "iteration 900 / 1500: loss 2.180174\n",
      "iteration 1000 / 1500: loss 2.158713\n",
      "iteration 1100 / 1500: loss 2.148191\n",
      "iteration 1200 / 1500: loss 2.132760\n",
      "iteration 1300 / 1500: loss 2.107279\n",
      "iteration 1400 / 1500: loss 2.113954\n",
      "iteration 0 / 1500: loss 1303.408833\n",
      "iteration 100 / 1500: loss 3.923475\n",
      "iteration 200 / 1500: loss 2.101827\n",
      "iteration 300 / 1500: loss 2.167871\n",
      "iteration 400 / 1500: loss 2.140009\n",
      "iteration 500 / 1500: loss 2.104451\n",
      "iteration 600 / 1500: loss 2.130354\n",
      "iteration 700 / 1500: loss 2.104959\n",
      "iteration 800 / 1500: loss 2.146215\n",
      "iteration 900 / 1500: loss 2.141552\n",
      "iteration 1000 / 1500: loss 2.108271\n",
      "iteration 1100 / 1500: loss 2.146059\n",
      "iteration 1200 / 1500: loss 2.109130\n",
      "iteration 1300 / 1500: loss 2.117149\n",
      "iteration 1400 / 1500: loss 2.134494\n",
      "iteration 0 / 1500: loss 1389.249240\n",
      "iteration 100 / 1500: loss 3.410178\n",
      "iteration 200 / 1500: loss 2.192620\n",
      "iteration 300 / 1500: loss 2.173974\n",
      "iteration 400 / 1500: loss 2.112647\n",
      "iteration 500 / 1500: loss 2.103040\n",
      "iteration 600 / 1500: loss 2.162878\n",
      "iteration 700 / 1500: loss 2.150076\n",
      "iteration 800 / 1500: loss 2.080255\n",
      "iteration 900 / 1500: loss 2.127015\n",
      "iteration 1000 / 1500: loss 2.088827\n",
      "iteration 1100 / 1500: loss 2.134786\n",
      "iteration 1200 / 1500: loss 2.070011\n",
      "iteration 1300 / 1500: loss 2.138814\n",
      "iteration 1400 / 1500: loss 2.126089\n",
      "iteration 0 / 1500: loss 1464.691650\n",
      "iteration 100 / 1500: loss 3.034933\n",
      "iteration 200 / 1500: loss 2.149367\n",
      "iteration 300 / 1500: loss 2.188879\n",
      "iteration 400 / 1500: loss 2.145579\n",
      "iteration 500 / 1500: loss 2.164265\n",
      "iteration 600 / 1500: loss 2.104719\n",
      "iteration 700 / 1500: loss 2.156125\n",
      "iteration 800 / 1500: loss 2.165040\n",
      "iteration 900 / 1500: loss 2.123278\n",
      "iteration 1000 / 1500: loss 2.115260\n",
      "iteration 1100 / 1500: loss 2.156543\n",
      "iteration 1200 / 1500: loss 2.096466\n",
      "iteration 1300 / 1500: loss 2.142788\n",
      "iteration 1400 / 1500: loss 2.075925\n",
      "iteration 0 / 1500: loss 773.280360\n",
      "iteration 100 / 1500: loss 13.003629\n",
      "iteration 200 / 1500: loss 2.284166\n",
      "iteration 300 / 1500: loss 2.082384\n",
      "iteration 400 / 1500: loss 2.093315\n",
      "iteration 500 / 1500: loss 2.054991\n",
      "iteration 600 / 1500: loss 2.057109\n",
      "iteration 700 / 1500: loss 2.056328\n",
      "iteration 800 / 1500: loss 2.092445\n",
      "iteration 900 / 1500: loss 2.116295\n",
      "iteration 1000 / 1500: loss 2.010293\n",
      "iteration 1100 / 1500: loss 2.121373\n",
      "iteration 1200 / 1500: loss 2.065611\n",
      "iteration 1300 / 1500: loss 2.071421\n",
      "iteration 1400 / 1500: loss 2.121844\n",
      "iteration 0 / 1500: loss 840.253820\n",
      "iteration 100 / 1500: loss 9.828590\n",
      "iteration 200 / 1500: loss 2.234096\n",
      "iteration 300 / 1500: loss 2.079066\n",
      "iteration 400 / 1500: loss 2.130813\n",
      "iteration 500 / 1500: loss 2.068608\n",
      "iteration 600 / 1500: loss 2.136059\n",
      "iteration 700 / 1500: loss 2.153787\n",
      "iteration 800 / 1500: loss 2.045451\n",
      "iteration 900 / 1500: loss 2.078290\n",
      "iteration 1000 / 1500: loss 2.100748\n",
      "iteration 1100 / 1500: loss 2.077261\n",
      "iteration 1200 / 1500: loss 2.111759\n",
      "iteration 1300 / 1500: loss 2.152502\n",
      "iteration 1400 / 1500: loss 2.073415\n",
      "iteration 0 / 1500: loss 931.086984\n",
      "iteration 100 / 1500: loss 7.669423\n",
      "iteration 200 / 1500: loss 2.170995\n",
      "iteration 300 / 1500: loss 2.116666\n",
      "iteration 400 / 1500: loss 2.143030\n",
      "iteration 500 / 1500: loss 2.102261\n",
      "iteration 600 / 1500: loss 2.125721\n",
      "iteration 700 / 1500: loss 2.081378\n",
      "iteration 800 / 1500: loss 2.136456\n",
      "iteration 900 / 1500: loss 2.121060\n",
      "iteration 1000 / 1500: loss 2.154910\n",
      "iteration 1100 / 1500: loss 2.113702\n",
      "iteration 1200 / 1500: loss 2.127874\n",
      "iteration 1300 / 1500: loss 2.119794\n",
      "iteration 1400 / 1500: loss 2.110044\n",
      "iteration 0 / 1500: loss 1000.821060\n",
      "iteration 100 / 1500: loss 5.983432\n",
      "iteration 200 / 1500: loss 2.131198\n",
      "iteration 300 / 1500: loss 2.140181\n",
      "iteration 400 / 1500: loss 2.140614\n",
      "iteration 500 / 1500: loss 2.068527\n",
      "iteration 600 / 1500: loss 2.139291\n",
      "iteration 700 / 1500: loss 2.101759\n",
      "iteration 800 / 1500: loss 2.116514\n",
      "iteration 900 / 1500: loss 2.218742\n",
      "iteration 1000 / 1500: loss 2.092829\n",
      "iteration 1100 / 1500: loss 2.118030\n",
      "iteration 1200 / 1500: loss 2.153685\n",
      "iteration 1300 / 1500: loss 2.138323\n",
      "iteration 1400 / 1500: loss 2.130550\n",
      "iteration 0 / 1500: loss 1088.867635\n",
      "iteration 100 / 1500: loss 4.901333\n",
      "iteration 200 / 1500: loss 2.109343\n",
      "iteration 300 / 1500: loss 2.155540\n",
      "iteration 400 / 1500: loss 2.165352\n",
      "iteration 500 / 1500: loss 2.092631\n",
      "iteration 600 / 1500: loss 2.097060\n",
      "iteration 700 / 1500: loss 2.111457\n",
      "iteration 800 / 1500: loss 2.073665\n",
      "iteration 900 / 1500: loss 2.110135\n",
      "iteration 1000 / 1500: loss 2.127112\n",
      "iteration 1100 / 1500: loss 2.092892\n",
      "iteration 1200 / 1500: loss 2.146006\n",
      "iteration 1300 / 1500: loss 2.117477\n",
      "iteration 1400 / 1500: loss 2.106768\n",
      "iteration 0 / 1500: loss 1151.343292\n",
      "iteration 100 / 1500: loss 4.022562\n",
      "iteration 200 / 1500: loss 2.127427\n",
      "iteration 300 / 1500: loss 2.126294\n",
      "iteration 400 / 1500: loss 2.122702\n",
      "iteration 500 / 1500: loss 2.117849\n",
      "iteration 600 / 1500: loss 2.125379\n",
      "iteration 700 / 1500: loss 2.113885\n",
      "iteration 800 / 1500: loss 2.142487\n",
      "iteration 900 / 1500: loss 2.087153\n",
      "iteration 1000 / 1500: loss 2.134232\n",
      "iteration 1100 / 1500: loss 2.110895\n",
      "iteration 1200 / 1500: loss 2.135106\n",
      "iteration 1300 / 1500: loss 2.112322\n",
      "iteration 1400 / 1500: loss 2.126876\n",
      "iteration 0 / 1500: loss 1217.632725\n",
      "iteration 100 / 1500: loss 3.396885\n",
      "iteration 200 / 1500: loss 2.125218\n",
      "iteration 300 / 1500: loss 2.136479\n",
      "iteration 400 / 1500: loss 2.071603\n",
      "iteration 500 / 1500: loss 2.159304\n",
      "iteration 600 / 1500: loss 2.185070\n",
      "iteration 700 / 1500: loss 2.157037\n",
      "iteration 800 / 1500: loss 2.127503\n",
      "iteration 900 / 1500: loss 2.160093\n",
      "iteration 1000 / 1500: loss 2.172271\n",
      "iteration 1100 / 1500: loss 2.101315\n",
      "iteration 1200 / 1500: loss 2.179975\n",
      "iteration 1300 / 1500: loss 2.108327\n",
      "iteration 1400 / 1500: loss 2.166052\n",
      "iteration 0 / 1500: loss 1322.112449\n",
      "iteration 100 / 1500: loss 3.039656\n",
      "iteration 200 / 1500: loss 2.187322\n",
      "iteration 300 / 1500: loss 2.170562\n",
      "iteration 400 / 1500: loss 2.112093\n",
      "iteration 500 / 1500: loss 2.125029\n",
      "iteration 600 / 1500: loss 2.125639\n",
      "iteration 700 / 1500: loss 2.135547\n",
      "iteration 800 / 1500: loss 2.185859\n",
      "iteration 900 / 1500: loss 2.127843\n",
      "iteration 1000 / 1500: loss 2.095071\n",
      "iteration 1100 / 1500: loss 2.115265\n",
      "iteration 1200 / 1500: loss 2.183519\n",
      "iteration 1300 / 1500: loss 2.158391\n",
      "iteration 1400 / 1500: loss 2.149283\n",
      "iteration 0 / 1500: loss 1399.784353\n",
      "iteration 100 / 1500: loss 2.773444\n",
      "iteration 200 / 1500: loss 2.137983\n",
      "iteration 300 / 1500: loss 2.086831\n",
      "iteration 400 / 1500: loss 2.089609\n",
      "iteration 500 / 1500: loss 2.123601\n",
      "iteration 600 / 1500: loss 2.107186\n",
      "iteration 700 / 1500: loss 2.138484\n",
      "iteration 800 / 1500: loss 2.102142\n",
      "iteration 900 / 1500: loss 2.154220\n",
      "iteration 1000 / 1500: loss 2.144147\n",
      "iteration 1100 / 1500: loss 2.156367\n",
      "iteration 1200 / 1500: loss 2.105749\n",
      "iteration 1300 / 1500: loss 2.080364\n",
      "iteration 1400 / 1500: loss 2.123796\n",
      "iteration 0 / 1500: loss 1481.868369\n",
      "iteration 100 / 1500: loss 2.512852\n",
      "iteration 200 / 1500: loss 2.114426\n",
      "iteration 300 / 1500: loss 2.139128\n",
      "iteration 400 / 1500: loss 2.155288\n",
      "iteration 500 / 1500: loss 2.145682\n",
      "iteration 600 / 1500: loss 2.123387\n",
      "iteration 700 / 1500: loss 2.127405\n",
      "iteration 800 / 1500: loss 2.173249\n",
      "iteration 900 / 1500: loss 2.174952\n",
      "iteration 1000 / 1500: loss 2.113702\n",
      "iteration 1100 / 1500: loss 2.143710\n",
      "iteration 1200 / 1500: loss 2.160449\n",
      "iteration 1300 / 1500: loss 2.125717\n",
      "iteration 1400 / 1500: loss 2.158161\n",
      "iteration 0 / 1500: loss 773.895591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 9.288531\n",
      "iteration 200 / 1500: loss 2.156373\n",
      "iteration 300 / 1500: loss 2.147477\n",
      "iteration 400 / 1500: loss 2.132502\n",
      "iteration 500 / 1500: loss 2.124225\n",
      "iteration 600 / 1500: loss 2.017063\n",
      "iteration 700 / 1500: loss 2.116300\n",
      "iteration 800 / 1500: loss 2.125248\n",
      "iteration 900 / 1500: loss 2.044547\n",
      "iteration 1000 / 1500: loss 2.083073\n",
      "iteration 1100 / 1500: loss 2.106925\n",
      "iteration 1200 / 1500: loss 2.080369\n",
      "iteration 1300 / 1500: loss 2.090382\n",
      "iteration 1400 / 1500: loss 2.098529\n",
      "iteration 0 / 1500: loss 852.949561\n",
      "iteration 100 / 1500: loss 7.103953\n",
      "iteration 200 / 1500: loss 2.172444\n",
      "iteration 300 / 1500: loss 2.144890\n",
      "iteration 400 / 1500: loss 2.046897\n",
      "iteration 500 / 1500: loss 2.068974\n",
      "iteration 600 / 1500: loss 2.089570\n",
      "iteration 700 / 1500: loss 2.174367\n",
      "iteration 800 / 1500: loss 2.109589\n",
      "iteration 900 / 1500: loss 2.070416\n",
      "iteration 1000 / 1500: loss 2.066076\n",
      "iteration 1100 / 1500: loss 2.066445\n",
      "iteration 1200 / 1500: loss 2.080245\n",
      "iteration 1300 / 1500: loss 2.104022\n",
      "iteration 1400 / 1500: loss 2.083481\n",
      "iteration 0 / 1500: loss 912.039017\n",
      "iteration 100 / 1500: loss 5.402970\n",
      "iteration 200 / 1500: loss 2.143385\n",
      "iteration 300 / 1500: loss 2.122964\n",
      "iteration 400 / 1500: loss 2.145966\n",
      "iteration 500 / 1500: loss 2.100784\n",
      "iteration 600 / 1500: loss 2.091964\n",
      "iteration 700 / 1500: loss 2.103788\n",
      "iteration 800 / 1500: loss 2.074020\n",
      "iteration 900 / 1500: loss 2.107547\n",
      "iteration 1000 / 1500: loss 2.103277\n",
      "iteration 1100 / 1500: loss 2.075407\n",
      "iteration 1200 / 1500: loss 2.034695\n",
      "iteration 1300 / 1500: loss 2.066988\n",
      "iteration 1400 / 1500: loss 2.062813\n",
      "iteration 0 / 1500: loss 1020.973364\n",
      "iteration 100 / 1500: loss 4.421411\n",
      "iteration 200 / 1500: loss 2.129381\n",
      "iteration 300 / 1500: loss 2.141101\n",
      "iteration 400 / 1500: loss 2.117832\n",
      "iteration 500 / 1500: loss 2.101268\n",
      "iteration 600 / 1500: loss 2.092246\n",
      "iteration 700 / 1500: loss 2.095278\n",
      "iteration 800 / 1500: loss 2.087887\n",
      "iteration 900 / 1500: loss 2.171213\n",
      "iteration 1000 / 1500: loss 2.129833\n",
      "iteration 1100 / 1500: loss 2.092748\n",
      "iteration 1200 / 1500: loss 2.102469\n",
      "iteration 1300 / 1500: loss 2.059825\n",
      "iteration 1400 / 1500: loss 2.095600\n",
      "iteration 0 / 1500: loss 1100.113492\n",
      "iteration 100 / 1500: loss 3.678392\n",
      "iteration 200 / 1500: loss 2.132630\n",
      "iteration 300 / 1500: loss 2.146169\n",
      "iteration 400 / 1500: loss 2.148804\n",
      "iteration 500 / 1500: loss 2.088828\n",
      "iteration 600 / 1500: loss 2.137123\n",
      "iteration 700 / 1500: loss 2.121184\n",
      "iteration 800 / 1500: loss 2.177532\n",
      "iteration 900 / 1500: loss 2.085853\n",
      "iteration 1000 / 1500: loss 2.110299\n",
      "iteration 1100 / 1500: loss 2.133682\n",
      "iteration 1200 / 1500: loss 2.129765\n",
      "iteration 1300 / 1500: loss 2.133496\n",
      "iteration 1400 / 1500: loss 2.145785\n",
      "iteration 0 / 1500: loss 1157.907832\n",
      "iteration 100 / 1500: loss 3.128222\n",
      "iteration 200 / 1500: loss 2.138456\n",
      "iteration 300 / 1500: loss 2.142670\n",
      "iteration 400 / 1500: loss 2.172622\n",
      "iteration 500 / 1500: loss 2.142228\n",
      "iteration 600 / 1500: loss 2.151566\n",
      "iteration 700 / 1500: loss 2.103090\n",
      "iteration 800 / 1500: loss 2.109058\n",
      "iteration 900 / 1500: loss 2.115359\n",
      "iteration 1000 / 1500: loss 2.075693\n",
      "iteration 1100 / 1500: loss 2.111060\n",
      "iteration 1200 / 1500: loss 2.175361\n",
      "iteration 1300 / 1500: loss 2.129940\n",
      "iteration 1400 / 1500: loss 2.098805\n",
      "iteration 0 / 1500: loss 1229.884658\n",
      "iteration 100 / 1500: loss 2.801840\n",
      "iteration 200 / 1500: loss 2.156806\n",
      "iteration 300 / 1500: loss 2.082252\n",
      "iteration 400 / 1500: loss 2.140070\n",
      "iteration 500 / 1500: loss 2.130065\n",
      "iteration 600 / 1500: loss 2.137496\n",
      "iteration 700 / 1500: loss 2.138240\n",
      "iteration 800 / 1500: loss 2.163903\n",
      "iteration 900 / 1500: loss 2.172236\n",
      "iteration 1000 / 1500: loss 2.158533\n",
      "iteration 1100 / 1500: loss 2.147273\n",
      "iteration 1200 / 1500: loss 2.141179\n",
      "iteration 1300 / 1500: loss 2.117683\n",
      "iteration 1400 / 1500: loss 2.147851\n",
      "iteration 0 / 1500: loss 1304.961154\n",
      "iteration 100 / 1500: loss 2.580305\n",
      "iteration 200 / 1500: loss 2.169297\n",
      "iteration 300 / 1500: loss 2.181420\n",
      "iteration 400 / 1500: loss 2.117324\n",
      "iteration 500 / 1500: loss 2.033689\n",
      "iteration 600 / 1500: loss 2.193168\n",
      "iteration 700 / 1500: loss 2.139885\n",
      "iteration 800 / 1500: loss 2.137546\n",
      "iteration 900 / 1500: loss 2.125156\n",
      "iteration 1000 / 1500: loss 2.149144\n",
      "iteration 1100 / 1500: loss 2.101069\n",
      "iteration 1200 / 1500: loss 2.128445\n",
      "iteration 1300 / 1500: loss 2.155840\n",
      "iteration 1400 / 1500: loss 2.133739\n",
      "iteration 0 / 1500: loss 1389.966679\n",
      "iteration 100 / 1500: loss 2.510336\n",
      "iteration 200 / 1500: loss 2.142235\n",
      "iteration 300 / 1500: loss 2.108911\n",
      "iteration 400 / 1500: loss 2.119723\n",
      "iteration 500 / 1500: loss 2.152570\n",
      "iteration 600 / 1500: loss 2.106017\n",
      "iteration 700 / 1500: loss 2.167613\n",
      "iteration 800 / 1500: loss 2.125035\n",
      "iteration 900 / 1500: loss 2.118404\n",
      "iteration 1000 / 1500: loss 2.161475\n",
      "iteration 1100 / 1500: loss 2.123262\n",
      "iteration 1200 / 1500: loss 2.151878\n",
      "iteration 1300 / 1500: loss 2.116668\n",
      "iteration 1400 / 1500: loss 2.137200\n",
      "iteration 0 / 1500: loss 1455.537029\n",
      "iteration 100 / 1500: loss 2.324777\n",
      "iteration 200 / 1500: loss 2.124334\n",
      "iteration 300 / 1500: loss 2.175317\n",
      "iteration 400 / 1500: loss 2.143741\n",
      "iteration 500 / 1500: loss 2.146222\n",
      "iteration 600 / 1500: loss 2.118960\n",
      "iteration 700 / 1500: loss 2.177372\n",
      "iteration 800 / 1500: loss 2.153561\n",
      "iteration 900 / 1500: loss 2.132588\n",
      "iteration 1000 / 1500: loss 2.117445\n",
      "iteration 1100 / 1500: loss 2.134379\n",
      "iteration 1200 / 1500: loss 2.120203\n",
      "iteration 1300 / 1500: loss 2.142412\n",
      "iteration 1400 / 1500: loss 2.117824\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.331551 val accuracy: 0.346000\n",
      "lr 1.000000e-07 reg 2.750000e+04 train accuracy: 0.322388 val accuracy: 0.341000\n",
      "lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.325531 val accuracy: 0.335000\n",
      "lr 1.000000e-07 reg 3.250000e+04 train accuracy: 0.324347 val accuracy: 0.343000\n",
      "lr 1.000000e-07 reg 3.500000e+04 train accuracy: 0.320306 val accuracy: 0.332000\n",
      "lr 1.000000e-07 reg 3.750000e+04 train accuracy: 0.313163 val accuracy: 0.328000\n",
      "lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.320714 val accuracy: 0.336000\n",
      "lr 1.000000e-07 reg 4.250000e+04 train accuracy: 0.312102 val accuracy: 0.327000\n",
      "lr 1.000000e-07 reg 4.500000e+04 train accuracy: 0.314469 val accuracy: 0.323000\n",
      "lr 1.000000e-07 reg 4.750000e+04 train accuracy: 0.301898 val accuracy: 0.319000\n",
      "lr 1.400000e-07 reg 2.500000e+04 train accuracy: 0.327041 val accuracy: 0.343000\n",
      "lr 1.400000e-07 reg 2.750000e+04 train accuracy: 0.324408 val accuracy: 0.347000\n",
      "lr 1.400000e-07 reg 3.000000e+04 train accuracy: 0.321592 val accuracy: 0.336000\n",
      "lr 1.400000e-07 reg 3.250000e+04 train accuracy: 0.315510 val accuracy: 0.335000\n",
      "lr 1.400000e-07 reg 3.500000e+04 train accuracy: 0.314510 val accuracy: 0.320000\n",
      "lr 1.400000e-07 reg 3.750000e+04 train accuracy: 0.322408 val accuracy: 0.329000\n",
      "lr 1.400000e-07 reg 4.000000e+04 train accuracy: 0.309816 val accuracy: 0.326000\n",
      "lr 1.400000e-07 reg 4.250000e+04 train accuracy: 0.314633 val accuracy: 0.332000\n",
      "lr 1.400000e-07 reg 4.500000e+04 train accuracy: 0.297918 val accuracy: 0.315000\n",
      "lr 1.400000e-07 reg 4.750000e+04 train accuracy: 0.305408 val accuracy: 0.319000\n",
      "lr 1.800000e-07 reg 2.500000e+04 train accuracy: 0.329306 val accuracy: 0.346000\n",
      "lr 1.800000e-07 reg 2.750000e+04 train accuracy: 0.327714 val accuracy: 0.347000\n",
      "lr 1.800000e-07 reg 3.000000e+04 train accuracy: 0.320408 val accuracy: 0.341000\n",
      "lr 1.800000e-07 reg 3.250000e+04 train accuracy: 0.314837 val accuracy: 0.332000\n",
      "lr 1.800000e-07 reg 3.500000e+04 train accuracy: 0.307449 val accuracy: 0.327000\n",
      "lr 1.800000e-07 reg 3.750000e+04 train accuracy: 0.302265 val accuracy: 0.318000\n",
      "lr 1.800000e-07 reg 4.000000e+04 train accuracy: 0.320163 val accuracy: 0.336000\n",
      "lr 1.800000e-07 reg 4.250000e+04 train accuracy: 0.309000 val accuracy: 0.326000\n",
      "lr 1.800000e-07 reg 4.500000e+04 train accuracy: 0.308714 val accuracy: 0.326000\n",
      "lr 1.800000e-07 reg 4.750000e+04 train accuracy: 0.308816 val accuracy: 0.325000\n",
      "lr 2.200000e-07 reg 2.500000e+04 train accuracy: 0.320551 val accuracy: 0.331000\n",
      "lr 2.200000e-07 reg 2.750000e+04 train accuracy: 0.324286 val accuracy: 0.342000\n",
      "lr 2.200000e-07 reg 3.000000e+04 train accuracy: 0.322816 val accuracy: 0.341000\n",
      "lr 2.200000e-07 reg 3.250000e+04 train accuracy: 0.316735 val accuracy: 0.337000\n",
      "lr 2.200000e-07 reg 3.500000e+04 train accuracy: 0.311959 val accuracy: 0.328000\n",
      "lr 2.200000e-07 reg 3.750000e+04 train accuracy: 0.318612 val accuracy: 0.321000\n",
      "lr 2.200000e-07 reg 4.000000e+04 train accuracy: 0.315490 val accuracy: 0.328000\n",
      "lr 2.200000e-07 reg 4.250000e+04 train accuracy: 0.302898 val accuracy: 0.319000\n",
      "lr 2.200000e-07 reg 4.500000e+04 train accuracy: 0.306143 val accuracy: 0.315000\n",
      "lr 2.200000e-07 reg 4.750000e+04 train accuracy: 0.299449 val accuracy: 0.316000\n",
      "lr 2.600000e-07 reg 2.500000e+04 train accuracy: 0.333408 val accuracy: 0.351000\n",
      "lr 2.600000e-07 reg 2.750000e+04 train accuracy: 0.323653 val accuracy: 0.339000\n",
      "lr 2.600000e-07 reg 3.000000e+04 train accuracy: 0.328959 val accuracy: 0.345000\n",
      "lr 2.600000e-07 reg 3.250000e+04 train accuracy: 0.313633 val accuracy: 0.326000\n",
      "lr 2.600000e-07 reg 3.500000e+04 train accuracy: 0.309306 val accuracy: 0.326000\n",
      "lr 2.600000e-07 reg 3.750000e+04 train accuracy: 0.317327 val accuracy: 0.335000\n",
      "lr 2.600000e-07 reg 4.000000e+04 train accuracy: 0.309143 val accuracy: 0.332000\n",
      "lr 2.600000e-07 reg 4.250000e+04 train accuracy: 0.308306 val accuracy: 0.327000\n",
      "lr 2.600000e-07 reg 4.500000e+04 train accuracy: 0.321061 val accuracy: 0.331000\n",
      "lr 2.600000e-07 reg 4.750000e+04 train accuracy: 0.305245 val accuracy: 0.324000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.327551 val accuracy: 0.345000\n",
      "lr 3.000000e-07 reg 2.750000e+04 train accuracy: 0.323306 val accuracy: 0.341000\n",
      "lr 3.000000e-07 reg 3.000000e+04 train accuracy: 0.311286 val accuracy: 0.331000\n",
      "lr 3.000000e-07 reg 3.250000e+04 train accuracy: 0.308755 val accuracy: 0.322000\n",
      "lr 3.000000e-07 reg 3.500000e+04 train accuracy: 0.310633 val accuracy: 0.330000\n",
      "lr 3.000000e-07 reg 3.750000e+04 train accuracy: 0.314041 val accuracy: 0.329000\n",
      "lr 3.000000e-07 reg 4.000000e+04 train accuracy: 0.314163 val accuracy: 0.323000\n",
      "lr 3.000000e-07 reg 4.250000e+04 train accuracy: 0.313959 val accuracy: 0.319000\n",
      "lr 3.000000e-07 reg 4.500000e+04 train accuracy: 0.317286 val accuracy: 0.320000\n",
      "lr 3.000000e-07 reg 4.750000e+04 train accuracy: 0.300143 val accuracy: 0.315000\n",
      "lr 3.400000e-07 reg 2.500000e+04 train accuracy: 0.323388 val accuracy: 0.344000\n",
      "lr 3.400000e-07 reg 2.750000e+04 train accuracy: 0.327184 val accuracy: 0.332000\n",
      "lr 3.400000e-07 reg 3.000000e+04 train accuracy: 0.323286 val accuracy: 0.344000\n",
      "lr 3.400000e-07 reg 3.250000e+04 train accuracy: 0.313980 val accuracy: 0.309000\n",
      "lr 3.400000e-07 reg 3.500000e+04 train accuracy: 0.323694 val accuracy: 0.334000\n",
      "lr 3.400000e-07 reg 3.750000e+04 train accuracy: 0.316816 val accuracy: 0.333000\n",
      "lr 3.400000e-07 reg 4.000000e+04 train accuracy: 0.315735 val accuracy: 0.332000\n",
      "lr 3.400000e-07 reg 4.250000e+04 train accuracy: 0.299612 val accuracy: 0.317000\n",
      "lr 3.400000e-07 reg 4.500000e+04 train accuracy: 0.309735 val accuracy: 0.332000\n",
      "lr 3.400000e-07 reg 4.750000e+04 train accuracy: 0.313735 val accuracy: 0.322000\n",
      "lr 3.800000e-07 reg 2.500000e+04 train accuracy: 0.333020 val accuracy: 0.348000\n",
      "lr 3.800000e-07 reg 2.750000e+04 train accuracy: 0.321694 val accuracy: 0.333000\n",
      "lr 3.800000e-07 reg 3.000000e+04 train accuracy: 0.316837 val accuracy: 0.324000\n",
      "lr 3.800000e-07 reg 3.250000e+04 train accuracy: 0.313306 val accuracy: 0.327000\n",
      "lr 3.800000e-07 reg 3.500000e+04 train accuracy: 0.317204 val accuracy: 0.325000\n",
      "lr 3.800000e-07 reg 3.750000e+04 train accuracy: 0.313857 val accuracy: 0.322000\n",
      "lr 3.800000e-07 reg 4.000000e+04 train accuracy: 0.296122 val accuracy: 0.317000\n",
      "lr 3.800000e-07 reg 4.250000e+04 train accuracy: 0.303694 val accuracy: 0.326000\n",
      "lr 3.800000e-07 reg 4.500000e+04 train accuracy: 0.310102 val accuracy: 0.327000\n",
      "lr 3.800000e-07 reg 4.750000e+04 train accuracy: 0.309041 val accuracy: 0.326000\n",
      "lr 4.200000e-07 reg 2.500000e+04 train accuracy: 0.330959 val accuracy: 0.337000\n",
      "lr 4.200000e-07 reg 2.750000e+04 train accuracy: 0.321816 val accuracy: 0.343000\n",
      "lr 4.200000e-07 reg 3.000000e+04 train accuracy: 0.326102 val accuracy: 0.326000\n",
      "lr 4.200000e-07 reg 3.250000e+04 train accuracy: 0.316755 val accuracy: 0.326000\n",
      "lr 4.200000e-07 reg 3.500000e+04 train accuracy: 0.318673 val accuracy: 0.340000\n",
      "lr 4.200000e-07 reg 3.750000e+04 train accuracy: 0.309531 val accuracy: 0.325000\n",
      "lr 4.200000e-07 reg 4.000000e+04 train accuracy: 0.316082 val accuracy: 0.326000\n",
      "lr 4.200000e-07 reg 4.250000e+04 train accuracy: 0.305102 val accuracy: 0.325000\n",
      "lr 4.200000e-07 reg 4.500000e+04 train accuracy: 0.305735 val accuracy: 0.323000\n",
      "lr 4.200000e-07 reg 4.750000e+04 train accuracy: 0.303388 val accuracy: 0.305000\n",
      "lr 4.600000e-07 reg 2.500000e+04 train accuracy: 0.328755 val accuracy: 0.342000\n",
      "lr 4.600000e-07 reg 2.750000e+04 train accuracy: 0.325612 val accuracy: 0.334000\n",
      "lr 4.600000e-07 reg 3.000000e+04 train accuracy: 0.319408 val accuracy: 0.332000\n",
      "lr 4.600000e-07 reg 3.250000e+04 train accuracy: 0.308286 val accuracy: 0.314000\n",
      "lr 4.600000e-07 reg 3.500000e+04 train accuracy: 0.318327 val accuracy: 0.336000\n",
      "lr 4.600000e-07 reg 3.750000e+04 train accuracy: 0.303837 val accuracy: 0.319000\n",
      "lr 4.600000e-07 reg 4.000000e+04 train accuracy: 0.308143 val accuracy: 0.331000\n",
      "lr 4.600000e-07 reg 4.250000e+04 train accuracy: 0.306408 val accuracy: 0.321000\n",
      "lr 4.600000e-07 reg 4.500000e+04 train accuracy: 0.304265 val accuracy: 0.311000\n",
      "lr 4.600000e-07 reg 4.750000e+04 train accuracy: 0.292633 val accuracy: 0.300000\n",
      "best validation accuracy achieved during cross-validation: 0.351000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "num_split = 10\n",
    "for i in range(num_split):\n",
    "    for j in range(num_split):\n",
    "        learning_rate_ij = learning_rates[0] + i*(learning_rates[1]-learning_rates[0])/num_split\n",
    "        regularization_strengths_ij = regularization_strengths[0] + j*(regularization_strengths[1]-regularization_strengths[0])/num_split\n",
    "        softmax = Softmax()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=learning_rate_ij, reg=regularization_strengths_ij,\n",
    "                      num_iters=1500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        accuracy_train = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        accuracy_val = np.mean(y_val == y_val_pred)\n",
    "        results[(learning_rate_ij,regularization_strengths_ij)] = (accuracy_train,accuracy_val)\n",
    "        \n",
    "        if accuracy_val > best_val:\n",
    "            best_val = accuracy_val\n",
    "            best_softmax = softmax\n",
    "        \n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.342000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24ba9d1cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
